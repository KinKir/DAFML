{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "import pandas as pd\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(filepath, label):\n",
    "    cells = []\n",
    "    labels = []\n",
    "    file = os.listdir(filepath)\n",
    "    for img in file:\n",
    "        try:\n",
    "            image = cv2.imread(filepath + img)\n",
    "            image_from_array = Image.fromarray(image, 'RGB')\n",
    "            size_image = image_from_array.resize((50, 50))\n",
    "            cells.append(np.array(size_image))\n",
    "            labels.append(label)\n",
    "        except AttributeError as e:\n",
    "            print('Skipping file: ', img, e)\n",
    "    print(len(cells), ' Data Points Read!')\n",
    "    return np.array(cells), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genesis_train(file):\n",
    "    \n",
    "    print('Reading Training Data')\n",
    "    \n",
    "    ParasitizedCells, ParasitizedLabels = readData(file + '/Parasitized/', 1)\n",
    "    UninfectedCells, UninfectedLabels  = readData(file + '/Uninfected/', 0)\n",
    "    Cells = np.concatenate((ParasitizedCells, UninfectedCells))\n",
    "    Labels = np.concatenate((ParasitizedLabels, UninfectedLabels))\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "    TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)\n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    s = np.arange(Cells.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Cells = Cells[s]\n",
    "    Labels = Labels[s]\n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(Labels))\n",
    "    len_data=len(Cells)\n",
    "    print(len_data, ' Data Points')\n",
    "    \n",
    "    (x_train,x_test)=Cells, TestCells\n",
    "    (y_train,y_test)=Labels, TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_train = x_train.astype('float32')/255 \n",
    "    x_test = x_test.astype('float32')/255\n",
    "    train_len=len(x_train)\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "#     model.summary()\n",
    "\n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Fit the model with min batch size as 50[can tune batch size to some factor of 2^power ] \n",
    "    model.fit(x_train, y_train, batch_size=100, epochs=5, verbose=1)\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./output.h5\")\n",
    "    return len_data, scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_train(file, d):\n",
    "    \n",
    "    print('Reading Training Data')\n",
    "    \n",
    "    ParasitizedCells, ParasitizedLabels = readData(file + '/Parasitized/', 1)\n",
    "    UninfectedCells, UninfectedLabels  = readData(file + '/Uninfected/', 0)\n",
    "    Cells = np.concatenate((ParasitizedCells, UninfectedCells))\n",
    "    Labels = np.concatenate((ParasitizedLabels, UninfectedLabels))\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "    TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)\n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    s = np.arange(Cells.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Cells = Cells[s]\n",
    "    Labels = Labels[s]\n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(Labels))\n",
    "    len_data=len(Cells)\n",
    "    print(len_data, ' Data Points')\n",
    "    \n",
    "    (x_train,x_test)=Cells, TestCells\n",
    "    (y_train,y_test)=Labels, TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_train = x_train.astype('float32')/255 \n",
    "    x_test = x_test.astype('float32')/255\n",
    "    train_len=len(x_train)\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "\n",
    "    model.load_weights(\"./output.h5\")\n",
    "    \n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Fit the model with min batch size as 50[can tune batch size to some factor of 2^power ] \n",
    "    model.fit(x_train, y_train, batch_size=100, epochs=5, verbose=1)\n",
    "    \n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/\" + str(d) + \".h5\")\n",
    "    return len_data, scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "686  Data Points Read!\n",
      "696  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1382  Data Points\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 2s 135ms/step - loss: 0.6728 - accuracy: 0.6027\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 2s 150ms/step - loss: 0.6296 - accuracy: 0.6469\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 2s 147ms/step - loss: 0.5789 - accuracy: 0.7142\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 2s 149ms/step - loss: 0.5440 - accuracy: 0.7315\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 2s 129ms/step - loss: 0.4903 - accuracy: 0.7721\n",
      "173/173 [==============================] - 3s 16ms/step - loss: 0.4749 - accuracy: 0.7900\n",
      "Loss:  0.47488918900489807\n",
      "Accuracy:  0.7899692058563232\n",
      "Reading Training Data\n",
      "528  Data Points Read!\n",
      "533  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1061  Data Points\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 1s 91ms/step - loss: 0.4673 - accuracy: 0.7879\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 1s 90ms/step - loss: 0.4264 - accuracy: 0.8162\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 1s 96ms/step - loss: 0.3726 - accuracy: 0.8530\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 1s 96ms/step - loss: 0.3147 - accuracy: 0.8699\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 1s 86ms/step - loss: 0.2670 - accuracy: 0.9057\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 0.3025 - accuracy: 0.8749\n",
      "Loss:  0.3024612367153168\n",
      "Accuracy:  0.8748868107795715\n",
      "Reading Training Data\n",
      "522  Data Points Read!\n",
      "528  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1050  Data Points\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 2s 167ms/step - loss: 0.5450 - accuracy: 0.7448\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 1s 100ms/step - loss: 0.4732 - accuracy: 0.7914\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 1s 126ms/step - loss: 0.4347 - accuracy: 0.8105\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 1s 91ms/step - loss: 0.3879 - accuracy: 0.8419\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 0.3488 - accuracy: 0.8724\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.3182 - accuracy: 0.8803\n",
      "Loss:  0.31824126839637756\n",
      "Accuracy:  0.8803186416625977\n",
      "Reading Training Data\n",
      "692  Data Points Read!\n",
      "655  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1347  Data Points\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 2s 122ms/step - loss: 0.5021 - accuracy: 0.7602\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 1s 102ms/step - loss: 0.4287 - accuracy: 0.8092\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 1s 100ms/step - loss: 0.3923 - accuracy: 0.8448\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 1s 102ms/step - loss: 0.3223 - accuracy: 0.8753\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 1s 102ms/step - loss: 0.2833 - accuracy: 0.8886\n",
      "173/173 [==============================] - 2s 12ms/step - loss: 0.2785 - accuracy: 0.8839\n",
      "Loss:  0.27846673130989075\n",
      "Accuracy:  0.8839398622512817\n",
      "Reading Training Data\n",
      "448  Data Points Read!\n",
      "410  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "858  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 0.5274 - accuracy: 0.7541\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.4711 - accuracy: 0.8019\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 0.4223 - accuracy: 0.8135\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 0.3792 - accuracy: 0.8415\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.3516 - accuracy: 0.8520\n",
      "173/173 [==============================] - 2s 13ms/step - loss: 0.3922 - accuracy: 0.8479\n",
      "Loss:  0.392183780670166\n",
      "Accuracy:  0.8479087352752686\n",
      "Reading Training Data\n",
      "838  Data Points Read!\n",
      "838  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1676  Data Points\n",
      "Epoch 1/5\n",
      "17/17 [==============================] - 2s 102ms/step - loss: 0.5155 - accuracy: 0.7572\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - 2s 107ms/step - loss: 0.4178 - accuracy: 0.8204\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - 2s 103ms/step - loss: 0.3393 - accuracy: 0.8646\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - 2s 104ms/step - loss: 0.2733 - accuracy: 0.9004\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.2165 - accuracy: 0.9189\n",
      "173/173 [==============================] - 2s 12ms/step - loss: 0.2611 - accuracy: 0.9015\n",
      "Loss:  0.26110416650772095\n",
      "Accuracy:  0.901502788066864\n",
      "Reading Training Data\n",
      "599  Data Points Read!\n",
      "567  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1166  Data Points\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 0.6068 - accuracy: 0.6930\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 0.4938 - accuracy: 0.7804\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 0.4533 - accuracy: 0.7985\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 0.4060 - accuracy: 0.8233\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 1s 121ms/step - loss: 0.3403 - accuracy: 0.8739\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.3295 - accuracy: 0.8725\n",
      "Loss:  0.3295148015022278\n",
      "Accuracy:  0.8725330233573914\n",
      "Reading Training Data\n",
      "418  Data Points Read!\n",
      "395  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "813  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 0.5564 - accuracy: 0.7306\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 108ms/step - loss: 0.4825 - accuracy: 0.7958\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 0.4358 - accuracy: 0.8266\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 161ms/step - loss: 0.3995 - accuracy: 0.8426\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.3645 - accuracy: 0.8610\n",
      "173/173 [==============================] - 2s 12ms/step - loss: 0.3681 - accuracy: 0.8647\n",
      "Loss:  0.3681491017341614\n",
      "Accuracy:  0.8647474050521851\n",
      "Reading Training Data\n",
      "716  Data Points Read!\n",
      "729  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1445  Data Points\n",
      "Epoch 1/5\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.5056 - accuracy: 0.7661\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.4308 - accuracy: 0.8187\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 1s 93ms/step - loss: 0.3585 - accuracy: 0.8526\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 1s 95ms/step - loss: 0.2907 - accuracy: 0.8858\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 1s 96ms/step - loss: 0.2397 - accuracy: 0.9038\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 0.2601 - accuracy: 0.8972\n",
      "Loss:  0.26010987162590027\n",
      "Accuracy:  0.8971573710441589\n",
      "Reading Training Data\n",
      "530  Data Points Read!\n",
      "572  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1102  Data Points\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.5228 - accuracy: 0.7486\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 1s 86ms/step - loss: 0.4758 - accuracy: 0.7976\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.4397 - accuracy: 0.8022\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.3850 - accuracy: 0.8348\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 1s 85ms/step - loss: 0.3386 - accuracy: 0.8548\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 0.2926 - accuracy: 0.8914\n",
      "Loss:  0.2926398813724518\n",
      "Accuracy:  0.8913633823394775\n",
      "Reading Training Data\n",
      "695  Data Points Read!\n",
      "701  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1396  Data Points\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 1s 88ms/step - loss: 0.4831 - accuracy: 0.7636\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 93ms/step - loss: 0.4166 - accuracy: 0.8152\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 1s 94ms/step - loss: 0.3536 - accuracy: 0.8739\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.2748 - accuracy: 0.8954\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 1s 98ms/step - loss: 0.2387 - accuracy: 0.9083\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 0.2687 - accuracy: 0.8852\n",
      "Loss:  0.26874953508377075\n",
      "Accuracy:  0.8852072954177856\n",
      "Reading Training Data\n",
      "557  Data Points Read!\n",
      "577  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1134  Data Points\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 88ms/step - loss: 0.5318 - accuracy: 0.7504\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.4674 - accuracy: 0.7972\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.4382 - accuracy: 0.8104\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 1s 97ms/step - loss: 0.3673 - accuracy: 0.8501\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 1s 93ms/step - loss: 0.3238 - accuracy: 0.8713\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 0.3197 - accuracy: 0.8570\n",
      "Loss:  0.3197242021560669\n",
      "Accuracy:  0.8569617867469788\n",
      "Reading Training Data\n",
      "827  Data Points Read!\n",
      "796  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1623  Data Points\n",
      "Epoch 1/5\n",
      "17/17 [==============================] - 2s 97ms/step - loss: 0.4885 - accuracy: 0.7745\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - 2s 91ms/step - loss: 0.3960 - accuracy: 0.8330\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.3302 - accuracy: 0.8657\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - 1s 85ms/step - loss: 0.2749 - accuracy: 0.8928\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - 1s 87ms/step - loss: 0.2064 - accuracy: 0.9224\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.2424 - accuracy: 0.8999\n",
      "Loss:  0.2423737496137619\n",
      "Accuracy:  0.8998732566833496\n",
      "Reading Training Data\n",
      "395  Data Points Read!\n",
      "425  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "820  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 89ms/step - loss: 0.5328 - accuracy: 0.7524\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.4699 - accuracy: 0.7927\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.4384 - accuracy: 0.8159\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.3966 - accuracy: 0.8439\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.3699 - accuracy: 0.8549\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 0.3773 - accuracy: 0.8334\n",
      "Loss:  0.3773098886013031\n",
      "Accuracy:  0.8334238529205322\n",
      "Reading Training Data\n",
      "513  Data Points Read!\n",
      "528  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1041  Data Points\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 1s 85ms/step - loss: 0.5402 - accuracy: 0.7397\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 1s 87ms/step - loss: 0.4722 - accuracy: 0.7896\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 1s 86ms/step - loss: 0.4161 - accuracy: 0.8117\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 1s 85ms/step - loss: 0.3830 - accuracy: 0.8396\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 1s 86ms/step - loss: 0.3187 - accuracy: 0.8770\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 0.3743 - accuracy: 0.8439\n",
      "Loss:  0.37429124116897583\n",
      "Accuracy:  0.843925416469574\n",
      "Reading Training Data\n",
      "284  Data Points Read!\n",
      "281  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "565  Data Points\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.4779 - accuracy: 0.7912\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 80ms/step - loss: 0.4262 - accuracy: 0.8195\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.4054 - accuracy: 0.8265\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 79ms/step - loss: 0.3605 - accuracy: 0.8673\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.3325 - accuracy: 0.8619\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 0.3776 - accuracy: 0.8428\n",
      "Loss:  0.3775901198387146\n",
      "Accuracy:  0.8428390622138977\n",
      "Reading Training Data\n",
      "412  Data Points Read!\n",
      "416  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "828  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 0.5149 - accuracy: 0.7729\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 0.4450 - accuracy: 0.8140\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 0.4095 - accuracy: 0.8152\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.3767 - accuracy: 0.8490\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.3314 - accuracy: 0.8635\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.3345 - accuracy: 0.8680\n",
      "Loss:  0.3345092833042145\n",
      "Accuracy:  0.8680065274238586\n",
      "Reading Training Data\n",
      "417  Data Points Read!\n",
      "414  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "831  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 121ms/step - loss: 0.5778 - accuracy: 0.7196\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 119ms/step - loss: 0.4772 - accuracy: 0.7894\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 107ms/step - loss: 0.4252 - accuracy: 0.8171\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 109ms/step - loss: 0.3969 - accuracy: 0.8448\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 114ms/step - loss: 0.3523 - accuracy: 0.8688\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.3821 - accuracy: 0.8769\n",
      "Loss:  0.38207849860191345\n",
      "Accuracy:  0.8768784999847412\n",
      "Reading Training Data\n",
      "269  Data Points Read!\n",
      "252  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "521  Data Points\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 1s 120ms/step - loss: 0.5068 - accuracy: 0.7466\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.4520 - accuracy: 0.8177\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.4429 - accuracy: 0.8234\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 0.4102 - accuracy: 0.8292\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 1s 104ms/step - loss: 0.3716 - accuracy: 0.8369\n",
      "173/173 [==============================] - 2s 14ms/step - loss: 0.4145 - accuracy: 0.8327\n",
      "Loss:  0.41446277499198914\n",
      "Accuracy:  0.8326995968818665\n",
      "Reading Training Data\n",
      "407  Data Points Read!\n",
      "407  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "814  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 0.5436 - accuracy: 0.7297\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 119ms/step - loss: 0.4842 - accuracy: 0.7862\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 0.4290 - accuracy: 0.8170\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.3888 - accuracy: 0.8342\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 0.3647 - accuracy: 0.8600\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.3668 - accuracy: 0.8772\n",
      "Loss:  0.3668276369571686\n",
      "Accuracy:  0.8772406578063965\n",
      "Reading Training Data\n",
      "286  Data Points Read!\n",
      "276  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "562  Data Points\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 1s 110ms/step - loss: 0.5430 - accuracy: 0.7562\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.4957 - accuracy: 0.7811\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 1s 102ms/step - loss: 0.4678 - accuracy: 0.7954\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 1s 100ms/step - loss: 0.4530 - accuracy: 0.8167\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 96ms/step - loss: 0.4328 - accuracy: 0.8149\n",
      "173/173 [==============================] - 2s 12ms/step - loss: 0.4287 - accuracy: 0.8169\n",
      "Loss:  0.42869099974632263\n",
      "Accuracy:  0.816947340965271\n"
     ]
    }
   ],
   "source": [
    "FLAccuracy = {}\n",
    "# FLAccuracy['Complete Dataset'] = genesis_train('./input/cell_images')\n",
    "FLAccuracy['Genesis'] = genesis_train('./input/fed/genesis')\n",
    "FLAccuracy['d1'] = update_train('./input/fed/d1', 'd1')\n",
    "FLAccuracy['d2'] = update_train('./input/fed/d2', 'd2')\n",
    "FLAccuracy['d3'] = update_train('./input/fed/d3', 'd3')\n",
    "FLAccuracy['d4'] = update_train('./input/fed/d4', 'd4')\n",
    "FLAccuracy['d5'] = update_train('./input/fed/d5', 'd5')\n",
    "FLAccuracy['d6'] = update_train('./input/fed/d6', 'd6')\n",
    "FLAccuracy['d7'] = update_train('./input/fed/d7', 'd7')\n",
    "FLAccuracy['d8'] = update_train('./input/fed/d8', 'd8')\n",
    "FLAccuracy['d9'] = update_train('./input/fed/d9', 'd9')\n",
    "FLAccuracy['d10'] = update_train('./input/fed/d10', 'd10')\n",
    "FLAccuracy['d11'] = update_train('./input/fed/d11', 'd11')\n",
    "FLAccuracy['d12'] = update_train('./input/fed/d12', 'd12')\n",
    "FLAccuracy['d13'] = update_train('./input/fed/d13', 'd13')\n",
    "FLAccuracy['d14'] = update_train('./input/fed/d14', 'd14')\n",
    "FLAccuracy['d15'] = update_train('./input/fed/d15', 'd15')\n",
    "FLAccuracy['d16'] = update_train('./input/fed/d16', 'd16')\n",
    "FLAccuracy['d17'] = update_train('./input/fed/d17', 'd17')\n",
    "FLAccuracy['d18'] = update_train('./input/fed/d18', 'd18')\n",
    "FLAccuracy['d19'] = update_train('./input/fed/d19', 'd19')\n",
    "FLAccuracy['d20'] = update_train('./input/fed/d20', 'd20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Genesis': (1382, 0.7899692058563232),\n",
       " 'd1': (1061, 0.8748868107795715),\n",
       " 'd2': (1050, 0.8803186416625977),\n",
       " 'd3': (1347, 0.8839398622512817),\n",
       " 'd4': (858, 0.8479087352752686),\n",
       " 'd5': (1676, 0.901502788066864),\n",
       " 'd6': (1166, 0.8725330233573914),\n",
       " 'd7': (813, 0.8647474050521851),\n",
       " 'd8': (1445, 0.8971573710441589),\n",
       " 'd9': (1102, 0.8913633823394775),\n",
       " 'd10': (1396, 0.8852072954177856),\n",
       " 'd11': (1134, 0.8569617867469788),\n",
       " 'd12': (1623, 0.8998732566833496),\n",
       " 'd13': (820, 0.8334238529205322),\n",
       " 'd14': (1041, 0.843925416469574),\n",
       " 'd15': (565, 0.8428390622138977),\n",
       " 'd16': (828, 0.8680065274238586),\n",
       " 'd17': (831, 0.8768784999847412),\n",
       " 'd18': (521, 0.8326995968818665),\n",
       " 'd19': (814, 0.8772406578063965),\n",
       " 'd20': (562, 0.816947340965271)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSize</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Genesis</th>\n",
       "      <td>1382</td>\n",
       "      <td>0.789969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <td>1061</td>\n",
       "      <td>0.874887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2</th>\n",
       "      <td>1050</td>\n",
       "      <td>0.880319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d3</th>\n",
       "      <td>1347</td>\n",
       "      <td>0.883940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4</th>\n",
       "      <td>858</td>\n",
       "      <td>0.847909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d5</th>\n",
       "      <td>1676</td>\n",
       "      <td>0.901503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d6</th>\n",
       "      <td>1166</td>\n",
       "      <td>0.872533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d7</th>\n",
       "      <td>813</td>\n",
       "      <td>0.864747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d8</th>\n",
       "      <td>1445</td>\n",
       "      <td>0.897157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d9</th>\n",
       "      <td>1102</td>\n",
       "      <td>0.891363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d10</th>\n",
       "      <td>1396</td>\n",
       "      <td>0.885207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d11</th>\n",
       "      <td>1134</td>\n",
       "      <td>0.856962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d12</th>\n",
       "      <td>1623</td>\n",
       "      <td>0.899873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d13</th>\n",
       "      <td>820</td>\n",
       "      <td>0.833424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d14</th>\n",
       "      <td>1041</td>\n",
       "      <td>0.843925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d15</th>\n",
       "      <td>565</td>\n",
       "      <td>0.842839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d16</th>\n",
       "      <td>828</td>\n",
       "      <td>0.868007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d17</th>\n",
       "      <td>831</td>\n",
       "      <td>0.876878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d18</th>\n",
       "      <td>521</td>\n",
       "      <td>0.832700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d19</th>\n",
       "      <td>814</td>\n",
       "      <td>0.877241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d20</th>\n",
       "      <td>562</td>\n",
       "      <td>0.816947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DataSize  Accuracy\n",
       "Genesis      1382  0.789969\n",
       "d1           1061  0.874887\n",
       "d2           1050  0.880319\n",
       "d3           1347  0.883940\n",
       "d4            858  0.847909\n",
       "d5           1676  0.901503\n",
       "d6           1166  0.872533\n",
       "d7            813  0.864747\n",
       "d8           1445  0.897157\n",
       "d9           1102  0.891363\n",
       "d10          1396  0.885207\n",
       "d11          1134  0.856962\n",
       "d12          1623  0.899873\n",
       "d13           820  0.833424\n",
       "d14          1041  0.843925\n",
       "d15           565  0.842839\n",
       "d16           828  0.868007\n",
       "d17           831  0.876878\n",
       "d18           521  0.832700\n",
       "d19           814  0.877241\n",
       "d20           562  0.816947"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAccuracyDF = pd.DataFrame.from_dict(FLAccuracy, orient='index', columns=['DataSize', 'Accuracy'])\n",
    "FLAccuracyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Genesis', 'd1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7', 'd8', 'd9', 'd10',\n",
       "       'd11', 'd12', 'd13', 'd14', 'd15', 'd16', 'd17', 'd18', 'd19', 'd20'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAccuracyDF.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points in this round:  22035\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for w in FLAccuracy:\n",
    "    if 'Complete' in w:\n",
    "        continue\n",
    "    n += FLAccuracy[w][0]\n",
    "print('Total number of data points in this round: ', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAccuracyDF['Weightage'] = FLAccuracyDF['DataSize'].apply(lambda x: x/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSize</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weightage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Genesis</th>\n",
       "      <td>1382</td>\n",
       "      <td>0.789969</td>\n",
       "      <td>0.062718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <td>1061</td>\n",
       "      <td>0.874887</td>\n",
       "      <td>0.048151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2</th>\n",
       "      <td>1050</td>\n",
       "      <td>0.880319</td>\n",
       "      <td>0.047651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d3</th>\n",
       "      <td>1347</td>\n",
       "      <td>0.883940</td>\n",
       "      <td>0.061130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4</th>\n",
       "      <td>858</td>\n",
       "      <td>0.847909</td>\n",
       "      <td>0.038938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d5</th>\n",
       "      <td>1676</td>\n",
       "      <td>0.901503</td>\n",
       "      <td>0.076061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d6</th>\n",
       "      <td>1166</td>\n",
       "      <td>0.872533</td>\n",
       "      <td>0.052916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d7</th>\n",
       "      <td>813</td>\n",
       "      <td>0.864747</td>\n",
       "      <td>0.036896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d8</th>\n",
       "      <td>1445</td>\n",
       "      <td>0.897157</td>\n",
       "      <td>0.065577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d9</th>\n",
       "      <td>1102</td>\n",
       "      <td>0.891363</td>\n",
       "      <td>0.050011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d10</th>\n",
       "      <td>1396</td>\n",
       "      <td>0.885207</td>\n",
       "      <td>0.063354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d11</th>\n",
       "      <td>1134</td>\n",
       "      <td>0.856962</td>\n",
       "      <td>0.051464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d12</th>\n",
       "      <td>1623</td>\n",
       "      <td>0.899873</td>\n",
       "      <td>0.073656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d13</th>\n",
       "      <td>820</td>\n",
       "      <td>0.833424</td>\n",
       "      <td>0.037214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d14</th>\n",
       "      <td>1041</td>\n",
       "      <td>0.843925</td>\n",
       "      <td>0.047243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d15</th>\n",
       "      <td>565</td>\n",
       "      <td>0.842839</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d16</th>\n",
       "      <td>828</td>\n",
       "      <td>0.868007</td>\n",
       "      <td>0.037577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d17</th>\n",
       "      <td>831</td>\n",
       "      <td>0.876878</td>\n",
       "      <td>0.037713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d18</th>\n",
       "      <td>521</td>\n",
       "      <td>0.832700</td>\n",
       "      <td>0.023644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d19</th>\n",
       "      <td>814</td>\n",
       "      <td>0.877241</td>\n",
       "      <td>0.036941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d20</th>\n",
       "      <td>562</td>\n",
       "      <td>0.816947</td>\n",
       "      <td>0.025505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DataSize  Accuracy  Weightage\n",
       "Genesis      1382  0.789969   0.062718\n",
       "d1           1061  0.874887   0.048151\n",
       "d2           1050  0.880319   0.047651\n",
       "d3           1347  0.883940   0.061130\n",
       "d4            858  0.847909   0.038938\n",
       "d5           1676  0.901503   0.076061\n",
       "d6           1166  0.872533   0.052916\n",
       "d7            813  0.864747   0.036896\n",
       "d8           1445  0.897157   0.065577\n",
       "d9           1102  0.891363   0.050011\n",
       "d10          1396  0.885207   0.063354\n",
       "d11          1134  0.856962   0.051464\n",
       "d12          1623  0.899873   0.073656\n",
       "d13           820  0.833424   0.037214\n",
       "d14          1041  0.843925   0.047243\n",
       "d15           565  0.842839   0.025641\n",
       "d16           828  0.868007   0.037577\n",
       "d17           831  0.876878   0.037713\n",
       "d18           521  0.832700   0.023644\n",
       "d19           814  0.877241   0.036941\n",
       "d20           562  0.816947   0.025505"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAccuracyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(weight, scaler):\n",
    "    scaledWeights = []\n",
    "    for i in range(len(weight)):\n",
    "        scaledWeights.append(scaler * weight[i])\n",
    "    return scaledWeights\n",
    "\n",
    "def getScaledWeight(d, scaler):\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    \n",
    "    fpath = \"./weights/\"+d+\".h5\"\n",
    "    model.load_weights(fpath)\n",
    "    weight = model.get_weights()\n",
    "    scaledWeight = scale(weight, scaler)\n",
    "\n",
    "    return scaledWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgWeights(scaledWeights):\n",
    "    avg = list()\n",
    "    for weight_list_tuple in zip(*scaledWeights):\n",
    "        layer_mean = tf.math.reduce_sum(weight_list_tuple, axis=0)\n",
    "        avg.append(layer_mean)\n",
    "    return avg\n",
    "\n",
    "def FedAvg(models):\n",
    "    \n",
    "    scaledWeights = []\n",
    "    for m in models:\n",
    "        scaledWeights.append(getScaledWeight(m, FLAccuracyDF.loc[m]['Weightage']))\n",
    "    avgWeight = avgWeights(scaledWeights)\n",
    "    return avgWeight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(2, 2, 3, 16), dtype=float32, numpy=\n",
      "array([[[[ 6.29709959e-02,  3.05600781e-02,  4.73186262e-02,\n",
      "          -4.84639332e-02, -1.09765619e-01, -2.40126792e-02,\n",
      "          -2.00001881e-01,  4.64794785e-03, -9.27647501e-02,\n",
      "          -1.66123241e-01,  2.24855661e-01, -7.29905367e-02,\n",
      "           1.65846452e-01, -1.77608833e-01,  8.64765197e-02,\n",
      "          -2.02627137e-01],\n",
      "         [ 1.22623229e-02, -2.69816726e-01, -8.74007195e-02,\n",
      "          -2.52788186e-01, -1.04290135e-01,  1.57856569e-01,\n",
      "          -2.61925399e-01, -1.27280667e-01,  1.26418844e-01,\n",
      "          -1.46524444e-01, -2.43654959e-02,  2.42929533e-01,\n",
      "          -2.60935754e-01,  2.79806722e-02,  1.43053783e-02,\n",
      "          -1.92507386e-01],\n",
      "         [-1.32187217e-01, -1.47404790e-01, -3.16053033e-02,\n",
      "           1.14077598e-01, -1.20645165e-01,  1.56122614e-02,\n",
      "           2.14044124e-01,  1.66435450e-01, -1.01623848e-01,\n",
      "          -2.04821333e-01, -9.87461675e-03, -2.57626027e-01,\n",
      "          -2.19773769e-01, -1.49482757e-01, -1.49648502e-01,\n",
      "           4.13678773e-02]],\n",
      "\n",
      "        [[ 4.09911200e-02, -2.55751640e-01,  5.51730059e-02,\n",
      "           1.51951760e-01, -2.08946750e-01, -1.78974658e-01,\n",
      "          -2.33739480e-01, -1.88674644e-01, -2.48390481e-01,\n",
      "          -8.12981203e-02, -1.72898650e-01, -5.53919859e-02,\n",
      "           1.58572555e-01, -1.22390337e-01,  2.73622870e-02,\n",
      "           4.19585668e-02],\n",
      "         [-8.34954083e-02,  2.52027541e-01, -1.80260316e-01,\n",
      "          -2.27775335e-01, -1.51616588e-01,  2.35146865e-01,\n",
      "          -2.38066748e-01,  1.14227705e-01,  2.09067702e-01,\n",
      "           4.44908515e-02,  9.89529341e-02, -1.02538820e-02,\n",
      "           1.72871754e-01,  1.98662151e-02, -6.45739734e-02,\n",
      "           8.96787941e-02],\n",
      "         [ 1.29124120e-01, -2.28320614e-01,  1.61692813e-01,\n",
      "           6.46706596e-02,  2.05930516e-01,  9.05648619e-02,\n",
      "          -2.87533049e-02,  6.75788745e-02,  1.68647338e-02,\n",
      "           4.18692678e-02,  2.26347685e-01,  1.76918477e-01,\n",
      "           1.28755435e-01, -1.25659034e-01, -1.95176736e-01,\n",
      "           2.46681631e-01]]],\n",
      "\n",
      "\n",
      "       [[[ 6.76263869e-02, -1.64116889e-01, -4.63423915e-02,\n",
      "           1.08722940e-01, -2.42184967e-01,  1.78869694e-01,\n",
      "           1.28165051e-01, -1.95564687e-01,  1.54643655e-01,\n",
      "          -8.88377205e-02,  8.41335952e-02, -2.23800123e-01,\n",
      "          -1.60318166e-01,  1.83068842e-01, -2.22699732e-01,\n",
      "           5.19412458e-02],\n",
      "         [-1.74145699e-01,  1.64810389e-01,  3.44512388e-02,\n",
      "          -1.51834458e-01,  2.44864017e-01, -2.07519397e-01,\n",
      "          -4.64312620e-02, -9.52167138e-02,  1.13456339e-01,\n",
      "           2.46316984e-01, -1.51859090e-01,  2.41635039e-01,\n",
      "           1.27022326e-01,  2.29096413e-01, -1.57743827e-01,\n",
      "           1.86367273e-01],\n",
      "         [ 2.29843587e-01, -2.43908539e-01, -2.01413512e-01,\n",
      "           9.97636020e-02,  2.12529823e-01,  6.52621761e-02,\n",
      "           2.25567147e-01, -1.94455549e-01,  5.37216440e-02,\n",
      "           9.14809331e-02, -1.86621517e-01,  2.08363146e-01,\n",
      "           1.42783493e-01, -1.71516404e-01,  2.87856683e-02,\n",
      "           7.70495385e-02]],\n",
      "\n",
      "        [[ 1.47195205e-01,  9.29638669e-02, -2.24783435e-01,\n",
      "           1.57111585e-01,  2.19253033e-01, -1.44216925e-01,\n",
      "           1.88499019e-01,  2.31585249e-01,  1.90144151e-01,\n",
      "          -9.07958075e-02, -4.38553058e-02,  1.75386280e-01,\n",
      "          -2.41612554e-01, -1.50471807e-01,  1.24751553e-01,\n",
      "          -1.57731324e-01],\n",
      "         [ 1.61211178e-01,  1.91906020e-01, -1.63448617e-01,\n",
      "          -1.06442757e-01, -7.65680149e-02, -5.57944737e-02,\n",
      "          -1.51635736e-01,  9.22414038e-05,  2.16817468e-01,\n",
      "          -1.81188583e-01,  1.44259129e-02, -8.36728215e-02,\n",
      "           1.69681653e-01,  1.00190096e-01,  1.78319663e-01,\n",
      "          -1.59951687e-01],\n",
      "         [-4.09052819e-02, -1.97755881e-02, -1.84581861e-01,\n",
      "          -1.45668117e-02, -1.44612893e-01,  2.52977610e-01,\n",
      "           1.50122315e-01, -2.20931277e-01,  3.89067531e-02,\n",
      "           2.37456664e-01,  1.47347704e-01, -4.30123396e-02,\n",
      "          -4.51557077e-02, -5.94768040e-02, -2.90106405e-02,\n",
      "          -2.41003498e-01]]]], dtype=float32)>, <tf.Tensor: shape=(16,), dtype=float32, numpy=\n",
      "array([-0.01007514, -0.001635  , -0.01025345, -0.00173561,  0.03268854,\n",
      "       -0.00208687,  0.00159445, -0.01452221, -0.00939484, -0.00810049,\n",
      "        0.00187787, -0.00531907, -0.01053663, -0.01091921, -0.01560304,\n",
      "       -0.00814741], dtype=float32)>, <tf.Tensor: shape=(2, 2, 16, 32), dtype=float32, numpy=\n",
      "array([[[[-0.09537445,  0.0238525 ,  0.08094858, ..., -0.11627594,\n",
      "           0.01361733, -0.00270094],\n",
      "         [ 0.03979139,  0.12243818, -0.10753282, ..., -0.02761981,\n",
      "           0.00115342, -0.11250883],\n",
      "         [-0.00319301,  0.03747182, -0.12272758, ..., -0.17002101,\n",
      "          -0.09323299, -0.09280318],\n",
      "         ...,\n",
      "         [ 0.01198404,  0.18341562, -0.08970661, ...,  0.07275134,\n",
      "           0.16204083, -0.11643461],\n",
      "         [ 0.11521284,  0.09957907, -0.06785449, ..., -0.16049376,\n",
      "          -0.03478961,  0.05059588],\n",
      "         [-0.10904927,  0.05084139, -0.11017702, ..., -0.13262197,\n",
      "          -0.09998756,  0.07306522]],\n",
      "\n",
      "        [[ 0.14296961,  0.14634573, -0.07494281, ...,  0.04981283,\n",
      "          -0.02226478, -0.00955457],\n",
      "         [-0.18278444, -0.12257586,  0.1105509 , ...,  0.06898673,\n",
      "          -0.14074963,  0.12880448],\n",
      "         [ 0.05062443,  0.09011479, -0.01778495, ...,  0.01384717,\n",
      "           0.05328264,  0.01165935],\n",
      "         ...,\n",
      "         [ 0.11653248,  0.15053472,  0.02755488, ..., -0.00180184,\n",
      "          -0.11342274,  0.02041092],\n",
      "         [ 0.11759499,  0.10058165,  0.1134244 , ...,  0.01960301,\n",
      "          -0.04442984, -0.04641576],\n",
      "         [-0.01923554, -0.1412403 , -0.0296952 , ...,  0.01371953,\n",
      "           0.06250753,  0.09004614]]],\n",
      "\n",
      "\n",
      "       [[[ 0.14302328, -0.03723493,  0.05975141, ..., -0.08695951,\n",
      "          -0.03743268, -0.16613564],\n",
      "         [ 0.04879532,  0.01573316,  0.03783104, ..., -0.07207087,\n",
      "           0.138336  ,  0.06230988],\n",
      "         [-0.17695434, -0.14341961,  0.1074056 , ..., -0.0374577 ,\n",
      "          -0.04587403, -0.06571089],\n",
      "         ...,\n",
      "         [ 0.08705768, -0.09674174,  0.15470633, ...,  0.07526341,\n",
      "           0.14084874, -0.07903387],\n",
      "         [ 0.11766817, -0.07612219, -0.03450292, ..., -0.17648341,\n",
      "          -0.08644895, -0.1104345 ],\n",
      "         [ 0.09279143, -0.14395934, -0.12362929, ..., -0.08168439,\n",
      "          -0.01325884,  0.06796352]],\n",
      "\n",
      "        [[-0.10889354,  0.09163773,  0.064237  , ..., -0.11828446,\n",
      "          -0.03442774,  0.11232536],\n",
      "         [ 0.1001339 ,  0.06223914, -0.13083588, ...,  0.05516066,\n",
      "          -0.17094746,  0.06631851],\n",
      "         [ 0.13773926,  0.10093588, -0.13178204, ..., -0.05654637,\n",
      "          -0.06447498,  0.10750612],\n",
      "         ...,\n",
      "         [ 0.06421222,  0.1465723 , -0.04136471, ...,  0.05052067,\n",
      "          -0.06008358, -0.12787679],\n",
      "         [ 0.04828766,  0.05224356, -0.16164221, ...,  0.10612825,\n",
      "          -0.11255513,  0.04035288],\n",
      "         [-0.16791548,  0.06760396, -0.13105269, ...,  0.06637175,\n",
      "           0.03395404,  0.00674641]]]], dtype=float32)>, <tf.Tensor: shape=(32,), dtype=float32, numpy=\n",
      "array([ 7.8103901e-03, -2.5253366e-03,  9.4068382e-05, -1.0054309e-02,\n",
      "        5.6395228e-03,  2.2935962e-02,  3.6243699e-04, -6.3295038e-03,\n",
      "        3.1995773e-02,  1.3141254e-02,  8.1295166e-03, -1.7962465e-03,\n",
      "       -5.8354164e-04, -7.3266337e-03, -8.7775812e-03,  7.3068980e-03,\n",
      "       -1.2754799e-03, -8.6430637e-03, -1.2573479e-02, -5.5672540e-03,\n",
      "       -1.1904450e-02, -4.4461302e-03, -1.9995815e-03, -3.0116516e-03,\n",
      "       -1.4999485e-02, -6.2715444e-03,  1.2082155e-02,  9.9959513e-03,\n",
      "       -3.4943717e-03,  4.8271101e-03, -4.3909899e-03, -7.2158724e-03],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(2, 2, 32, 64), dtype=float32, numpy=\n",
      "array([[[[-0.05097382, -0.103765  ,  0.05987458, ..., -0.05191575,\n",
      "          -0.00475351,  0.11364381],\n",
      "         [-0.00168266, -0.03842274, -0.07855407, ..., -0.0831281 ,\n",
      "          -0.0727298 ,  0.05130759],\n",
      "         [-0.05923396, -0.01078768,  0.02821486, ...,  0.02914551,\n",
      "          -0.06302543, -0.08857528],\n",
      "         ...,\n",
      "         [-0.04730515, -0.01813016,  0.05795706, ..., -0.10225762,\n",
      "           0.01166681,  0.09502172],\n",
      "         [-0.09534874, -0.05579872, -0.04130651, ..., -0.13656466,\n",
      "           0.09485541, -0.11062077],\n",
      "         [-0.05683818, -0.04426903, -0.03588507, ..., -0.09614032,\n",
      "           0.03641687, -0.04682203]],\n",
      "\n",
      "        [[ 0.16425094,  0.00555054,  0.00494457, ...,  0.00898783,\n",
      "           0.10889069,  0.08391576],\n",
      "         [-0.02203994, -0.05366965, -0.11370585, ...,  0.02346958,\n",
      "          -0.05212817,  0.04323069],\n",
      "         [-0.04689383, -0.03736854, -0.01105933, ...,  0.05111279,\n",
      "          -0.10411535, -0.07766384],\n",
      "         ...,\n",
      "         [ 0.00388702, -0.05325527, -0.08057582, ..., -0.09036542,\n",
      "           0.08295781,  0.02695396],\n",
      "         [-0.06440552, -0.01092236, -0.04536002, ..., -0.07186652,\n",
      "           0.0181105 ,  0.08185989],\n",
      "         [-0.0419886 , -0.03008262, -0.05637935, ...,  0.04211152,\n",
      "           0.02888373, -0.06597874]]],\n",
      "\n",
      "\n",
      "       [[[-0.03833229, -0.10916876,  0.1284822 , ..., -0.05868948,\n",
      "          -0.12631544,  0.12959538],\n",
      "         [ 0.04788401,  0.03303488,  0.06145496, ..., -0.09839627,\n",
      "           0.03922866,  0.09826408],\n",
      "         [-0.07018621,  0.10417556, -0.02953644, ...,  0.05394573,\n",
      "           0.0176542 , -0.01720765],\n",
      "         ...,\n",
      "         [ 0.05617737, -0.05888639, -0.09679458, ..., -0.0468474 ,\n",
      "           0.08244231,  0.10589295],\n",
      "         [ 0.0106239 , -0.01670274, -0.12942371, ..., -0.0671926 ,\n",
      "          -0.02569613, -0.07168184],\n",
      "         [ 0.08028605,  0.07828661, -0.0697    , ..., -0.11891251,\n",
      "           0.01813116,  0.09038625]],\n",
      "\n",
      "        [[ 0.08789235, -0.01428154,  0.06888534, ...,  0.09440541,\n",
      "          -0.03437117, -0.10806488],\n",
      "         [-0.06862493,  0.04934879, -0.04362699, ..., -0.07955035,\n",
      "           0.05497896,  0.09841695],\n",
      "         [-0.10580587, -0.00517587,  0.0524519 , ..., -0.00403526,\n",
      "           0.02082359,  0.03606828],\n",
      "         ...,\n",
      "         [ 0.1386118 , -0.05597519, -0.08895131, ..., -0.04376757,\n",
      "          -0.04135654,  0.1048066 ],\n",
      "         [ 0.1007139 ,  0.05583981, -0.03825749, ..., -0.11412786,\n",
      "           0.01438947, -0.04390105],\n",
      "         [ 0.00362133,  0.09658727, -0.03246778, ...,  0.05341358,\n",
      "           0.02820618, -0.01911995]]]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=float32, numpy=\n",
      "array([-8.1379199e-03,  6.1860224e-03,  1.6091123e-02, -6.1926767e-03,\n",
      "       -6.7583038e-03, -9.4345007e-03, -6.0022073e-03,  3.3236535e-03,\n",
      "        1.1789045e-02, -8.9633754e-03,  6.6024857e-03, -2.8087604e-03,\n",
      "       -5.3437511e-03, -5.8863903e-03, -1.5286321e-02, -8.4907245e-03,\n",
      "       -4.6209171e-03, -1.0798765e-02,  1.3004292e-02,  7.3197032e-03,\n",
      "       -2.3899684e-03, -5.9073414e-03,  1.2828208e-03,  8.9652007e-03,\n",
      "       -1.0949758e-02,  1.0419855e-03, -3.2625971e-03,  3.0798630e-03,\n",
      "       -2.5548907e-03, -9.3173067e-04, -9.3546556e-03,  1.8120963e-03,\n",
      "       -3.8350022e-03, -1.4084974e-02, -3.6110198e-03, -4.5017223e-03,\n",
      "       -2.4619487e-03,  1.8209267e-02, -2.8989587e-03,  1.6913021e-03,\n",
      "       -5.3030080e-03, -5.9248605e-03,  1.1082625e-03, -3.8173990e-03,\n",
      "        1.8847305e-03, -8.2327640e-03, -1.7164005e-03,  2.6117486e-05,\n",
      "       -4.1099940e-03,  1.6918827e-02, -5.7169464e-03, -2.2128168e-03,\n",
      "        4.6205113e-04, -5.6640753e-03, -2.1636956e-03, -4.5968518e-03,\n",
      "       -1.8414368e-03, -1.2497818e-02, -7.9843001e-03,  2.1397090e-03,\n",
      "       -3.7410008e-03,  3.0753077e-03, -9.8327342e-03, -6.4184298e-03],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(2304, 500), dtype=float32, numpy=\n",
      "array([[-1.5044750e-02, -1.9083844e-02,  4.4547915e-02, ...,\n",
      "         2.8617123e-02, -3.0377548e-02,  4.0829502e-05],\n",
      "       [ 3.3865925e-02, -2.2863064e-02, -1.0030516e-02, ...,\n",
      "         1.5757322e-02, -1.9825639e-02, -1.3909874e-02],\n",
      "       [-1.5962871e-02,  1.8267710e-02,  1.4149060e-03, ...,\n",
      "        -4.8543759e-02,  2.1052580e-02, -1.3631902e-02],\n",
      "       ...,\n",
      "       [-1.1880083e-02, -9.0174275e-03, -1.0868900e-02, ...,\n",
      "         6.0880934e-03, -3.2359600e-02, -1.5150492e-02],\n",
      "       [-1.4395309e-02,  1.9456392e-02,  1.5710445e-02, ...,\n",
      "         5.6687263e-03,  8.9815492e-03,  5.4982271e-02],\n",
      "       [ 1.5056844e-02, -3.3522487e-02, -1.8322496e-02, ...,\n",
      "        -3.1811804e-02,  2.7293323e-02,  3.3484075e-02]], dtype=float32)>, <tf.Tensor: shape=(500,), dtype=float32, numpy=\n",
      "array([-5.53416880e-03,  1.12668620e-02, -2.51924153e-03, -4.17344924e-03,\n",
      "        9.82279144e-03, -1.77115912e-03, -6.46361941e-03, -5.59850084e-03,\n",
      "        1.04833283e-02, -3.00975540e-03, -8.56306928e-04, -5.20168711e-03,\n",
      "        8.98524467e-03,  1.28810275e-02, -4.06666519e-03, -3.82398628e-03,\n",
      "       -5.54959150e-03, -3.64668365e-03, -7.92631507e-03, -6.75905682e-03,\n",
      "       -6.28078589e-03, -3.44249629e-03,  8.80345237e-03,  9.11579560e-03,\n",
      "       -3.39526846e-03, -8.37900490e-03, -3.30694043e-03, -5.70433028e-03,\n",
      "       -6.35400740e-03, -6.88481797e-03, -1.85096229e-03, -5.58825349e-03,\n",
      "        1.12006404e-02, -5.44621237e-03, -2.14726408e-03, -4.09880653e-03,\n",
      "       -4.19273693e-03, -5.61053818e-03, -4.17135097e-03, -4.33228025e-03,\n",
      "        9.04962793e-03,  1.30762206e-02, -5.75472182e-03,  1.13392835e-02,\n",
      "       -5.61683020e-03, -4.13291575e-03, -3.34750861e-03, -4.06149169e-03,\n",
      "       -3.66749568e-03, -3.24321515e-03,  9.33618098e-03, -7.22316233e-03,\n",
      "        9.70353745e-03, -3.68067622e-03, -6.16257172e-03,  8.20684154e-03,\n",
      "       -4.89435019e-03, -2.14906642e-04, -4.98367054e-03,  8.90939869e-03,\n",
      "        1.21129975e-02, -5.61448559e-03,  1.06549365e-02, -5.29230293e-03,\n",
      "       -7.12551922e-03, -3.38179525e-03, -1.93966873e-04,  1.35435369e-02,\n",
      "       -2.14829901e-03, -5.71408169e-03, -5.53259440e-03,  1.09508727e-02,\n",
      "        7.46000465e-03,  8.89100228e-03,  1.11365514e-02,  1.13110561e-02,\n",
      "       -3.79776186e-03, -2.44536810e-03,  8.80534574e-03,  6.33697840e-04,\n",
      "        1.35701690e-02, -3.75823537e-03,  8.84520821e-03,  1.01940446e-02,\n",
      "        1.18791331e-02, -4.45917575e-03, -5.21652494e-03,  1.00889653e-02,\n",
      "        1.04786623e-02, -1.21432624e-03, -4.65361541e-03, -4.73555177e-03,\n",
      "       -2.50295829e-03, -6.23297552e-03,  1.23668527e-02, -5.57112740e-03,\n",
      "       -5.67774940e-03, -8.34269915e-03,  1.95718429e-04, -7.68144522e-03,\n",
      "        1.63959491e-03, -7.55467964e-03, -6.72129123e-03, -5.64150047e-03,\n",
      "       -4.72860225e-03, -2.58226250e-03,  1.13667669e-02, -4.72749490e-03,\n",
      "       -6.87813736e-04, -7.16909440e-03,  1.38499383e-02, -4.85397922e-03,\n",
      "       -4.04685503e-03, -8.88858456e-03, -3.92845739e-03, -5.49211912e-03,\n",
      "        8.65745731e-03, -6.40754169e-03,  8.97939131e-03, -7.40244985e-03,\n",
      "       -4.83700447e-03, -7.29304156e-04, -3.94110195e-03, -6.02743588e-04,\n",
      "        1.11664683e-02, -5.32353530e-03, -2.82435399e-03, -3.30526684e-03,\n",
      "       -6.55194139e-03, -6.93755038e-03, -1.95045010e-04, -4.21771314e-03,\n",
      "       -2.50204583e-03, -3.01466952e-03, -6.63121603e-03, -4.80043842e-03,\n",
      "       -7.46097416e-03,  1.32271247e-02, -5.97341685e-03, -5.82827302e-03,\n",
      "       -3.68164736e-03, -6.31515449e-03,  9.53376200e-03, -5.07155526e-03,\n",
      "       -2.95582996e-03,  1.04220407e-02,  1.18372804e-02, -2.46381853e-03,\n",
      "       -4.80389362e-03, -5.61798224e-03, -5.24300430e-03,  1.54450033e-02,\n",
      "        1.32732317e-02, -4.70723212e-03,  1.08032888e-02, -6.82017487e-03,\n",
      "       -6.28149649e-03, -2.42553814e-03, -8.88851658e-03, -5.68734854e-03,\n",
      "       -4.40417882e-03, -4.07085195e-03,  1.10611124e-02,  8.81560147e-03,\n",
      "        1.23010194e-02,  1.01457806e-02, -2.99730501e-03,  1.13221137e-02,\n",
      "       -5.19194733e-03, -4.76593524e-03, -3.49803339e-03, -5.18953474e-03,\n",
      "        1.03672050e-04, -2.19414127e-03, -6.27203798e-03, -4.62104147e-03,\n",
      "        9.67853703e-03,  1.19852079e-02, -4.77695186e-03, -3.02255899e-03,\n",
      "       -3.44580598e-03, -5.03656501e-03,  1.16737988e-02, -1.01109478e-03,\n",
      "        1.50007810e-02,  1.32154869e-02, -5.56759816e-03, -5.62080368e-03,\n",
      "        1.20012723e-02, -3.59317590e-03, -4.95124375e-03,  1.28721436e-02,\n",
      "        8.88990797e-03, -6.95590302e-03, -3.73836188e-03, -9.03419685e-03,\n",
      "       -7.40891788e-03, -4.48821840e-04, -3.66688869e-03, -3.19493329e-03,\n",
      "        1.01427659e-02,  1.11271814e-02,  1.10702636e-02, -2.11332203e-03,\n",
      "        2.97213136e-03,  1.19747408e-02, -5.24236402e-03, -1.82243134e-03,\n",
      "       -5.73743833e-03, -6.53023412e-03,  8.42849817e-03, -1.75264548e-03,\n",
      "       -3.80321289e-03,  1.21859685e-02, -5.28530916e-03,  8.93038046e-03,\n",
      "       -1.85946876e-03, -4.21086838e-03, -4.17282851e-03,  9.28098708e-03,\n",
      "        1.21705001e-02, -3.06760427e-03,  9.03287902e-03,  1.28004504e-02,\n",
      "        5.14049828e-03, -5.60991513e-03, -5.60030155e-03,  1.09166838e-02,\n",
      "       -1.18235371e-03, -1.28952414e-03, -4.58185887e-03, -5.13173454e-03,\n",
      "       -3.54259741e-03,  1.11334594e-02, -3.29254218e-03, -2.63589993e-03,\n",
      "       -5.60598960e-03, -8.67103878e-03, -2.93730665e-03,  2.26680830e-04,\n",
      "       -5.56087401e-03, -3.64702521e-03, -1.47829112e-03,  1.20302821e-02,\n",
      "       -3.81234381e-03, -7.11362157e-03,  1.12991966e-02, -9.81083792e-03,\n",
      "       -3.74554598e-04, -7.74306990e-03,  1.20938942e-03, -7.87460152e-03,\n",
      "       -4.29497054e-03, -3.82512342e-03, -5.61306300e-03, -5.59601514e-03,\n",
      "       -4.65118355e-04,  1.30068157e-02, -6.96778996e-04, -7.90145155e-03,\n",
      "       -4.68531577e-03, -6.41325163e-03, -9.63451690e-04, -1.03702885e-03,\n",
      "       -2.45038699e-03, -9.89039708e-03,  1.02759171e-02, -5.18792914e-03,\n",
      "        3.90443206e-03, -5.46140177e-03, -5.85449254e-03, -7.75552588e-03,\n",
      "       -2.65215989e-03,  2.57740874e-04, -4.21085767e-03,  9.33959056e-03,\n",
      "       -8.25493410e-03,  8.85015074e-03,  1.23032620e-02, -2.43170653e-03,\n",
      "       -4.67009097e-03,  9.55339614e-03, -6.68688398e-03,  1.02293398e-02,\n",
      "       -2.21140636e-03, -2.83968076e-03, -6.47431705e-03, -1.17618497e-03,\n",
      "       -2.06996733e-03, -3.74172023e-03,  8.99831671e-03,  1.34283733e-02,\n",
      "       -2.95580830e-03, -2.78668827e-03, -5.36573119e-03,  1.10625606e-02,\n",
      "        1.20213265e-02,  1.29257431e-02,  1.02709904e-02,  1.46592576e-02,\n",
      "       -2.87306751e-03, -2.57530925e-03, -1.21975131e-03, -1.98024209e-03,\n",
      "       -3.65329836e-03, -3.85023374e-03,  1.08012799e-02,  9.48995803e-05,\n",
      "       -5.53057902e-03,  1.33074366e-03,  8.31840653e-03, -5.04636159e-03,\n",
      "       -2.35966896e-03,  1.21791437e-02, -8.25849071e-04,  3.88702890e-03,\n",
      "        1.00835385e-02, -2.26927036e-03,  1.11112688e-02,  1.07091153e-02,\n",
      "       -4.75936569e-03, -5.16986288e-03, -1.54961017e-04, -4.29618731e-03,\n",
      "       -2.62062997e-03, -6.55703200e-03,  1.31747909e-02,  9.15726181e-03,\n",
      "       -6.96338667e-03, -6.86828885e-03, -2.18273280e-03, -1.80473831e-03,\n",
      "       -5.26898634e-03, -5.49727399e-03,  1.12049617e-02, -6.60270732e-03,\n",
      "       -5.09557826e-03,  9.42141470e-03, -6.21194812e-03, -5.80852665e-03,\n",
      "       -2.03451491e-03, -1.75885821e-03,  1.20100761e-02,  9.89157520e-03,\n",
      "        1.17626321e-02,  1.30208796e-02, -9.12684627e-05,  1.14509659e-02,\n",
      "        9.74610634e-03, -2.73874239e-03, -4.58180439e-03, -4.26019076e-03,\n",
      "       -3.16335331e-03,  2.72267079e-03, -5.57976356e-03, -3.09378584e-03,\n",
      "       -3.35071399e-03, -4.05666325e-03,  1.09317005e-02, -5.98766655e-03,\n",
      "       -1.52193604e-03, -4.38027596e-03, -6.51899609e-04, -3.89595074e-03,\n",
      "       -3.28053813e-03,  1.11519285e-02, -2.93060159e-03,  9.72441025e-03,\n",
      "       -8.85107648e-03,  1.08384751e-02,  1.17983613e-02, -6.72848150e-03,\n",
      "        1.06212078e-02, -5.12874452e-03, -3.57506657e-03, -5.36638824e-03,\n",
      "       -4.11110371e-03, -5.96203050e-03, -2.85185175e-03, -7.19294325e-03,\n",
      "       -7.81475846e-03,  9.83515382e-03, -9.24775959e-04, -7.53887743e-03,\n",
      "       -3.22614377e-03,  1.22755012e-02,  1.05221616e-02, -9.61643644e-04,\n",
      "        1.17060253e-02, -3.14895273e-03,  1.15561383e-02, -4.81133629e-03,\n",
      "       -2.31146999e-03,  7.01590907e-03, -3.40830768e-04, -6.03314582e-03,\n",
      "        9.08868946e-03, -3.73258116e-03, -1.99783803e-03, -2.79753609e-03,\n",
      "       -8.24323739e-04,  1.19561637e-02, -3.95200495e-03, -1.26268726e-03,\n",
      "       -1.55346445e-03, -6.04792219e-03,  7.40239490e-03, -2.95300642e-03,\n",
      "        9.86943301e-03,  1.24818468e-02, -2.34776968e-03, -6.97968761e-03,\n",
      "       -2.50496319e-03,  6.33505266e-03, -2.07575876e-03, -1.63455214e-03,\n",
      "        1.00206258e-02, -2.48630135e-03, -3.58426711e-03, -3.98963783e-03,\n",
      "        1.01473331e-02,  1.10968156e-02, -4.35665529e-03,  1.21512776e-02,\n",
      "       -6.98138727e-03, -9.01398889e-04, -5.60927298e-03, -1.45288149e-03,\n",
      "       -3.87207884e-03, -2.94308970e-03,  1.10045495e-02, -3.27208196e-03,\n",
      "        7.61404494e-03, -5.47929248e-03, -3.01020127e-03, -9.55906138e-03,\n",
      "       -5.53280767e-03, -7.28675572e-04,  5.55282598e-03, -1.76397408e-03,\n",
      "        6.81685284e-03,  1.01069128e-02, -5.46161924e-03, -1.28993904e-03,\n",
      "       -1.05295400e-03, -3.52117140e-03, -5.76895103e-03,  2.91664759e-03,\n",
      "       -1.51595997e-03,  7.78692542e-03, -1.12719163e-02, -3.46895959e-03,\n",
      "        1.00675672e-02, -8.27851531e-04, -5.40378690e-03, -5.57581801e-03,\n",
      "       -7.43054925e-03, -3.99085227e-03, -2.47309078e-03, -3.02337669e-03,\n",
      "       -2.71165813e-03, -3.62162798e-04, -5.38837304e-03, -4.36798111e-03,\n",
      "       -1.64058746e-03, -7.32781878e-03,  1.01796910e-02, -5.91279287e-03,\n",
      "        9.76460241e-03, -2.69726431e-03, -5.62081765e-03, -2.54856143e-03,\n",
      "       -4.46600746e-03, -4.22631390e-03, -9.64728184e-04,  1.30091254e-02,\n",
      "        1.24156959e-02, -2.78786267e-03,  9.98625811e-03,  1.08450977e-02,\n",
      "        8.82346649e-03,  1.14490669e-02, -5.76793309e-03,  9.75369290e-03,\n",
      "       -4.55533341e-03,  2.60111806e-03,  1.26153072e-02, -3.95245338e-03,\n",
      "        2.82460329e-04, -1.97257055e-03,  9.04539507e-03, -5.60731255e-03,\n",
      "       -3.03759193e-03, -6.28925627e-03, -4.76786727e-03,  1.24086421e-02,\n",
      "       -2.92672310e-03, -6.27733255e-03, -4.93518915e-03, -4.00497485e-03],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(500, 2), dtype=float32, numpy=\n",
      "array([[ 0.05311327,  0.09514155],\n",
      "       [ 0.09232233, -0.02851534],\n",
      "       [-0.08964266,  0.04173108],\n",
      "       [ 0.08367732, -0.0923907 ],\n",
      "       [ 0.07473528, -0.08274475],\n",
      "       [-0.0627539 ,  0.09992003],\n",
      "       [-0.0911191 , -0.07911095],\n",
      "       [-0.0498883 , -0.09269869],\n",
      "       [ 0.07364915, -0.06293585],\n",
      "       [ 0.04505345,  0.08532324],\n",
      "       [ 0.03102643,  0.07411335],\n",
      "       [ 0.03223044, -0.05786802],\n",
      "       [ 0.10995429, -0.01073887],\n",
      "       [ 0.10785273,  0.00285203],\n",
      "       [ 0.04749208,  0.05066685],\n",
      "       [ 0.0555957 ,  0.09664138],\n",
      "       [-0.00818688,  0.0174538 ],\n",
      "       [-0.08163548, -0.05619446],\n",
      "       [ 0.02267669, -0.04380619],\n",
      "       [-0.01865696,  0.0669779 ],\n",
      "       [ 0.05982472,  0.05374481],\n",
      "       [ 0.05908397, -0.05340671],\n",
      "       [ 0.04229654, -0.0705682 ],\n",
      "       [ 0.12624112, -0.03905471],\n",
      "       [-0.08749062, -0.01966353],\n",
      "       [ 0.02720188,  0.02096189],\n",
      "       [ 0.06169368,  0.08842259],\n",
      "       [ 0.01870921, -0.08792486],\n",
      "       [-0.05300749, -0.06052515],\n",
      "       [ 0.05659251, -0.05551624],\n",
      "       [-0.00416558,  0.06536365],\n",
      "       [-0.01034767,  0.05329547],\n",
      "       [ 0.07533468, -0.05162623],\n",
      "       [ 0.03910884,  0.06408792],\n",
      "       [-0.02865157,  0.03982799],\n",
      "       [-0.00720344,  0.03931104],\n",
      "       [ 0.03563602,  0.00198104],\n",
      "       [ 0.0744648 , -0.02432105],\n",
      "       [-0.09741265,  0.04142449],\n",
      "       [-0.02363664, -0.05258123],\n",
      "       [ 0.12679069, -0.04927159],\n",
      "       [-0.02962878, -0.12274925],\n",
      "       [-0.08189599, -0.02983104],\n",
      "       [ 0.08228916, -0.12541004],\n",
      "       [-0.01287102,  0.03614439],\n",
      "       [-0.016284  ,  0.06752599],\n",
      "       [ 0.01914192,  0.04822027],\n",
      "       [ 0.09245484, -0.08001135],\n",
      "       [-0.04274287, -0.00859715],\n",
      "       [ 0.05542322,  0.00793769],\n",
      "       [ 0.03698973, -0.11074325],\n",
      "       [ 0.05847845, -0.01122535],\n",
      "       [ 0.05873084, -0.06315666],\n",
      "       [-0.01872551,  0.0537422 ],\n",
      "       [ 0.00085652, -0.0669762 ],\n",
      "       [ 0.09326971, -0.04454394],\n",
      "       [ 0.03200875,  0.03614467],\n",
      "       [ 0.04222843,  0.09592429],\n",
      "       [-0.0204555 ,  0.00569754],\n",
      "       [ 0.05601981, -0.12443047],\n",
      "       [ 0.00828085, -0.09333592],\n",
      "       [-0.05703217, -0.02206847],\n",
      "       [ 0.07337331, -0.01721186],\n",
      "       [-0.10884454, -0.00482925],\n",
      "       [ 0.0913316 ,  0.08112097],\n",
      "       [-0.07608335,  0.00085231],\n",
      "       [-0.09819711, -0.05532588],\n",
      "       [ 0.07308225,  0.00092411],\n",
      "       [-0.05119678,  0.05604082],\n",
      "       [ 0.07414087,  0.10505526],\n",
      "       [ 0.01682709,  0.01961546],\n",
      "       [ 0.08240177, -0.10032496],\n",
      "       [ 0.04001589, -0.08403035],\n",
      "       [ 0.12239482,  0.06419824],\n",
      "       [ 0.1324751 , -0.01727679],\n",
      "       [-0.02381351, -0.10001826],\n",
      "       [-0.06213731,  0.07949966],\n",
      "       [-0.04335392,  0.01332419],\n",
      "       [ 0.04419218, -0.06350309],\n",
      "       [-0.04222876,  0.10688141],\n",
      "       [ 0.07051704, -0.01126709],\n",
      "       [-0.05057632,  0.00396998],\n",
      "       [ 0.13414493, -0.005617  ],\n",
      "       [ 0.12455928,  0.00826711],\n",
      "       [ 0.12764968, -0.07120249],\n",
      "       [-0.05176536,  0.07963897],\n",
      "       [ 0.08401894,  0.03100447],\n",
      "       [ 0.08777595, -0.04021717],\n",
      "       [ 0.08009473, -0.10042297],\n",
      "       [ 0.05165022,  0.10122991],\n",
      "       [-0.02704764, -0.05683092],\n",
      "       [-0.07406341, -0.05763072],\n",
      "       [-0.07700881,  0.11013646],\n",
      "       [ 0.08613678,  0.01541759],\n",
      "       [ 0.10774246, -0.0432528 ],\n",
      "       [-0.01273213,  0.0070588 ],\n",
      "       [-0.01267115,  0.09487985],\n",
      "       [-0.01893573, -0.01735934],\n",
      "       [ 0.07963271,  0.09316237],\n",
      "       [-0.0849181 ,  0.09335002],\n",
      "       [-0.07189753,  0.06319249],\n",
      "       [-0.01437461, -0.07791349],\n",
      "       [-0.07779831, -0.03652881],\n",
      "       [-0.04608301,  0.05867525],\n",
      "       [-0.08924111,  0.09480068],\n",
      "       [-0.10260601, -0.02373477],\n",
      "       [ 0.11865773, -0.01351118],\n",
      "       [ 0.0479991 , -0.04756196],\n",
      "       [-0.00401142,  0.09675419],\n",
      "       [ 0.00615512, -0.02635094],\n",
      "       [ 0.03999988, -0.05102275],\n",
      "       [ 0.07417063,  0.09568918],\n",
      "       [-0.10557904, -0.06227076],\n",
      "       [ 0.00559694, -0.06304982],\n",
      "       [ 0.06008749,  0.0375339 ],\n",
      "       [-0.06326835,  0.02782372],\n",
      "       [ 0.12035009, -0.09589054],\n",
      "       [ 0.00579257,  0.08006887],\n",
      "       [-0.01508083, -0.09803493],\n",
      "       [ 0.03642597, -0.00953708],\n",
      "       [ 0.00085658,  0.05022113],\n",
      "       [-0.0886467 , -0.03710546],\n",
      "       [-0.03850847,  0.05948083],\n",
      "       [-0.10435861,  0.10078683],\n",
      "       [ 0.10006507,  0.07087514],\n",
      "       [ 0.00495187,  0.01239195],\n",
      "       [-0.0808455 ,  0.05980099],\n",
      "       [-0.07445095,  0.10324261],\n",
      "       [ 0.0441043 ,  0.0076606 ],\n",
      "       [-0.07697976,  0.07268078],\n",
      "       [-0.07653762, -0.01193937],\n",
      "       [-0.09638125, -0.07475944],\n",
      "       [-0.06650464, -0.03050037],\n",
      "       [-0.05667973,  0.08996452],\n",
      "       [ 0.05176068, -0.0103077 ],\n",
      "       [ 0.03983513,  0.04569188],\n",
      "       [-0.01864344,  0.08519334],\n",
      "       [ 0.0079924 , -0.1436909 ],\n",
      "       [-0.06566814, -0.07663368],\n",
      "       [-0.00131069, -0.07904355],\n",
      "       [ 0.00564205,  0.04069302],\n",
      "       [-0.01365841, -0.06190141],\n",
      "       [ 0.09065706, -0.04532593],\n",
      "       [-0.07778468,  0.06040839],\n",
      "       [ 0.07657637,  0.00619171],\n",
      "       [ 0.12044221, -0.05069051],\n",
      "       [ 0.10355301, -0.01523891],\n",
      "       [-0.05932459, -0.02066591],\n",
      "       [-0.04601621,  0.05558309],\n",
      "       [-0.02698389,  0.07053442],\n",
      "       [ 0.07882285, -0.04121957],\n",
      "       [ 0.06135241, -0.0304644 ],\n",
      "       [ 0.09189945,  0.00352685],\n",
      "       [ 0.07324959,  0.00517631],\n",
      "       [ 0.00789318, -0.08145713],\n",
      "       [ 0.02677604,  0.06527786],\n",
      "       [-0.06949059, -0.07066917],\n",
      "       [-0.06701694,  0.00092053],\n",
      "       [-0.00497779, -0.07953849],\n",
      "       [-0.0283235 ,  0.03393211],\n",
      "       [-0.00527409,  0.03929835],\n",
      "       [-0.03482571,  0.04214784],\n",
      "       [ 0.13532552, -0.0626914 ],\n",
      "       [ 0.04993704, -0.08098684],\n",
      "       [ 0.07999712, -0.06564064],\n",
      "       [ 0.07224462,  0.00101267],\n",
      "       [-0.08284859, -0.05882983],\n",
      "       [ 0.0772713 , -0.02075527],\n",
      "       [ 0.09509362, -0.04933422],\n",
      "       [ 0.00703417, -0.05456049],\n",
      "       [ 0.06934486,  0.10816067],\n",
      "       [ 0.02769655,  0.03253336],\n",
      "       [-0.02737729,  0.08668435],\n",
      "       [-0.04420803,  0.08174775],\n",
      "       [ 0.01564788,  0.04019317],\n",
      "       [-0.07019652, -0.04801892],\n",
      "       [ 0.12309071, -0.02950791],\n",
      "       [ 0.12868382, -0.12376357],\n",
      "       [-0.0832961 , -0.07362361],\n",
      "       [-0.05780158, -0.06723551],\n",
      "       [-0.04076702,  0.08101598],\n",
      "       [-0.08256809, -0.05024244],\n",
      "       [ 0.02858803, -0.08623272],\n",
      "       [ 0.05343185,  0.06022738],\n",
      "       [ 0.04897041, -0.12582912],\n",
      "       [ 0.07588698, -0.04684179],\n",
      "       [-0.09575359, -0.05143378],\n",
      "       [-0.00919765,  0.08940119],\n",
      "       [ 0.04028715, -0.05920773],\n",
      "       [-0.08991487, -0.06903067],\n",
      "       [-0.10565867, -0.07443611],\n",
      "       [ 0.05306325, -0.04204487],\n",
      "       [ 0.11833616, -0.04841996],\n",
      "       [-0.00116225, -0.01993424],\n",
      "       [ 0.03919767, -0.02504915],\n",
      "       [-0.09598167, -0.09569343],\n",
      "       [-0.05948884,  0.0847825 ],\n",
      "       [-0.07438079,  0.0150445 ],\n",
      "       [ 0.00054377,  0.04857477],\n",
      "       [ 0.05947721, -0.00051871],\n",
      "       [-0.00574352, -0.08031838],\n",
      "       [ 0.03730416, -0.0494786 ],\n",
      "       [-0.00285258, -0.10425571],\n",
      "       [-0.06427557,  0.08026918],\n",
      "       [ 0.09416965,  0.03169006],\n",
      "       [ 0.09222102, -0.04166833],\n",
      "       [ 0.00919792,  0.0091698 ],\n",
      "       [-0.04828366, -0.06778532],\n",
      "       [-0.00445858, -0.00255824],\n",
      "       [ 0.0004533 , -0.01015858],\n",
      "       [ 0.10160092, -0.04120492],\n",
      "       [-0.03873373, -0.00724124],\n",
      "       [ 0.05889171, -0.07536945],\n",
      "       [ 0.00992343, -0.12579918],\n",
      "       [ 0.09181193,  0.07637873],\n",
      "       [ 0.12409923, -0.00582372],\n",
      "       [-0.02855872,  0.08005448],\n",
      "       [ 0.00997379,  0.0599444 ],\n",
      "       [-0.01220272,  0.04842511],\n",
      "       [ 0.12037931,  0.0039286 ],\n",
      "       [ 0.06772764, -0.13433436],\n",
      "       [-0.08197935, -0.06232351],\n",
      "       [ 0.06975443, -0.06791086],\n",
      "       [ 0.06771931, -0.0870792 ],\n",
      "       [ 0.10175464, -0.05467491],\n",
      "       [-0.08869459, -0.03781105],\n",
      "       [ 0.03419706, -0.02091313],\n",
      "       [ 0.03717361, -0.05176476],\n",
      "       [ 0.03977473,  0.0802637 ],\n",
      "       [ 0.0786541 , -0.07467738],\n",
      "       [ 0.06886227,  0.06881069],\n",
      "       [-0.0448051 , -0.04030641],\n",
      "       [ 0.03404806,  0.10317541],\n",
      "       [ 0.07326619, -0.06823327],\n",
      "       [-0.03777633,  0.10320984],\n",
      "       [-0.10982945,  0.09765613],\n",
      "       [ 0.09580883,  0.00511308],\n",
      "       [ 0.00609537, -0.02871921],\n",
      "       [ 0.01074211,  0.0428532 ],\n",
      "       [ 0.05898644, -0.04757763],\n",
      "       [ 0.08945356, -0.04275674],\n",
      "       [-0.0443637 , -0.00082853],\n",
      "       [-0.05223919,  0.0191266 ],\n",
      "       [ 0.10952595, -0.02627584],\n",
      "       [-0.09769403,  0.04058051],\n",
      "       [ 0.09491251, -0.07337349],\n",
      "       [ 0.09357981, -0.07139876],\n",
      "       [-0.08782497, -0.08310542],\n",
      "       [-0.02649409, -0.00852232],\n",
      "       [-0.03283405, -0.02455583],\n",
      "       [ 0.06076952,  0.01649073],\n",
      "       [-0.04753735, -0.04494003],\n",
      "       [ 0.03921098, -0.09155199],\n",
      "       [ 0.0690461 ,  0.07946862],\n",
      "       [-0.09138182,  0.00620584],\n",
      "       [ 0.07899081,  0.0901005 ],\n",
      "       [-0.01421051,  0.06142241],\n",
      "       [ 0.01492564, -0.09382761],\n",
      "       [-0.09812864,  0.06189889],\n",
      "       [-0.03061358, -0.01388041],\n",
      "       [-0.08795238,  0.06047294],\n",
      "       [ 0.07104512, -0.05074662],\n",
      "       [-0.01304952, -0.085006  ],\n",
      "       [-0.0283862 ,  0.06999809],\n",
      "       [ 0.01572364,  0.07186005],\n",
      "       [ 0.02568827, -0.00206702],\n",
      "       [ 0.08745052, -0.05517298],\n",
      "       [ 0.05046008, -0.0602025 ],\n",
      "       [-0.03640335, -0.10797785],\n",
      "       [ 0.08553269, -0.02292269],\n",
      "       [-0.03778361,  0.06580836],\n",
      "       [-0.06589452, -0.06769627],\n",
      "       [-0.03918728,  0.01647712],\n",
      "       [-0.0237859 ,  0.09259149],\n",
      "       [ 0.0724097 ,  0.07180366],\n",
      "       [ 0.1075801 , -0.02824615],\n",
      "       [ 0.0331415 ,  0.01933192],\n",
      "       [ 0.00501336, -0.07279122],\n",
      "       [-0.00366991, -0.10399261],\n",
      "       [-0.07857811,  0.0787955 ],\n",
      "       [-0.07753023,  0.05651722],\n",
      "       [ 0.07203721, -0.12172461],\n",
      "       [ 0.08254173, -0.0240918 ],\n",
      "       [ 0.01095617, -0.0724567 ],\n",
      "       [-0.0530875 ,  0.03137068],\n",
      "       [-0.0066072 ,  0.06545337],\n",
      "       [ 0.07457578, -0.0442367 ],\n",
      "       [ 0.05588755,  0.10217611],\n",
      "       [-0.02981724,  0.04941597],\n",
      "       [ 0.05253782, -0.01042885],\n",
      "       [ 0.10137051, -0.09172978],\n",
      "       [ 0.08790664,  0.02524709],\n",
      "       [-0.04589577,  0.07517756],\n",
      "       [ 0.08342946,  0.09162924],\n",
      "       [-0.08593765,  0.06982628],\n",
      "       [ 0.10655744, -0.05617983],\n",
      "       [ 0.06092152, -0.12207716],\n",
      "       [ 0.13419878, -0.02252891],\n",
      "       [ 0.09834116,  0.03958232],\n",
      "       [ 0.08437857,  0.01460592],\n",
      "       [ 0.0879554 , -0.01700424],\n",
      "       [ 0.00131226,  0.09671631],\n",
      "       [-0.07696921, -0.00924925],\n",
      "       [-0.00744633,  0.06004588],\n",
      "       [ 0.05903571,  0.07920177],\n",
      "       [-0.06260628, -0.09104627],\n",
      "       [-0.01939411, -0.11509704],\n",
      "       [-0.08755022, -0.04919781],\n",
      "       [ 0.08280303,  0.10103364],\n",
      "       [-0.09067247,  0.010084  ],\n",
      "       [ 0.06435605, -0.10556137],\n",
      "       [ 0.09159714, -0.06761235],\n",
      "       [-0.03296948,  0.08572628],\n",
      "       [ 0.11521567,  0.05843401],\n",
      "       [-0.11013817,  0.04792359],\n",
      "       [ 0.08529602, -0.09699252],\n",
      "       [ 0.09589008, -0.07699554],\n",
      "       [-0.04429969, -0.01039457],\n",
      "       [-0.04172556, -0.1274764 ],\n",
      "       [ 0.01776977, -0.09997707],\n",
      "       [ 0.03752356, -0.09220051],\n",
      "       [-0.0466691 , -0.06307165],\n",
      "       [-0.10223605,  0.06639349],\n",
      "       [ 0.0373287 ,  0.06885269],\n",
      "       [-0.1011653 ,  0.01160435],\n",
      "       [ 0.08323063,  0.030745  ],\n",
      "       [ 0.05227034, -0.04873146],\n",
      "       [ 0.02358136, -0.07874562],\n",
      "       [ 0.06542907,  0.04180999],\n",
      "       [ 0.05650794, -0.09472515],\n",
      "       [-0.00625252,  0.05431107],\n",
      "       [ 0.00902724,  0.05882132],\n",
      "       [ 0.08372131,  0.10223008],\n",
      "       [ 0.02161542,  0.02079195],\n",
      "       [ 0.05030798, -0.11809568],\n",
      "       [ 0.07213019,  0.02946462],\n",
      "       [-0.02988404,  0.09049877],\n",
      "       [ 0.09042504, -0.07592832],\n",
      "       [-0.09396608, -0.0944846 ],\n",
      "       [-0.02128671,  0.00555106],\n",
      "       [ 0.0881349 , -0.01951319],\n",
      "       [ 0.09815294,  0.02091831],\n",
      "       [ 0.06768715, -0.00866784],\n",
      "       [ 0.03008059, -0.03210432],\n",
      "       [ 0.07697138, -0.10169055],\n",
      "       [ 0.01389083, -0.1269208 ],\n",
      "       [-0.10234262, -0.05354223],\n",
      "       [ 0.10600115,  0.018374  ],\n",
      "       [ 0.00389425, -0.09662258],\n",
      "       [-0.06471124,  0.07431647],\n",
      "       [ 0.08016909,  0.10446185],\n",
      "       [ 0.05589481,  0.0695638 ],\n",
      "       [-0.05375014, -0.03225301],\n",
      "       [ 0.0376894 ,  0.00021482],\n",
      "       [-0.07243066, -0.03724802],\n",
      "       [-0.08446896, -0.00804608],\n",
      "       [-0.01091738,  0.03005325],\n",
      "       [ 0.00425963,  0.09999651],\n",
      "       [ 0.0634597 , -0.01699882],\n",
      "       [ 0.0536304 , -0.02552321],\n",
      "       [-0.09320226,  0.12028917],\n",
      "       [ 0.01233296,  0.02663163],\n",
      "       [-0.07040945,  0.07650659],\n",
      "       [ 0.04895047, -0.04399431],\n",
      "       [-0.02182859,  0.06619221],\n",
      "       [ 0.13631342, -0.0910524 ],\n",
      "       [-0.02438866,  0.04361154],\n",
      "       [ 0.0196631 , -0.0924203 ],\n",
      "       [-0.02257976,  0.02791588],\n",
      "       [ 0.10096157, -0.10916837],\n",
      "       [ 0.0721221 , -0.13175704],\n",
      "       [-0.03534862, -0.08653457],\n",
      "       [ 0.09404437, -0.08704901],\n",
      "       [ 0.09220192, -0.0581664 ],\n",
      "       [ 0.0577516 ,  0.05761899],\n",
      "       [ 0.02303884,  0.07061604],\n",
      "       [-0.07656746,  0.04161585],\n",
      "       [ 0.06183554,  0.00855878],\n",
      "       [-0.08804803, -0.03012522],\n",
      "       [-0.06404053,  0.09230365],\n",
      "       [ 0.05869053, -0.08241767],\n",
      "       [ 0.05102699, -0.05720156],\n",
      "       [-0.07484718, -0.02034107],\n",
      "       [-0.05531253, -0.0525405 ],\n",
      "       [ 0.06994882,  0.1004664 ],\n",
      "       [ 0.12005012, -0.03424909],\n",
      "       [ 0.03128378, -0.06865919],\n",
      "       [-0.04526852,  0.09185734],\n",
      "       [ 0.15100084, -0.01347744],\n",
      "       [ 0.00988858, -0.08292833],\n",
      "       [ 0.03963636, -0.10915481],\n",
      "       [-0.03419256, -0.01783324],\n",
      "       [-0.08656282, -0.03493471],\n",
      "       [-0.05074825, -0.09571009],\n",
      "       [-0.03827269,  0.07175969],\n",
      "       [ 0.07545518,  0.01717809],\n",
      "       [ 0.07358796,  0.02868232],\n",
      "       [ 0.06777788,  0.02454507],\n",
      "       [-0.08414111, -0.00173827],\n",
      "       [-0.09479126,  0.07037915],\n",
      "       [ 0.0302516 ,  0.10241633],\n",
      "       [ 0.01278188, -0.11881058],\n",
      "       [ 0.00672977,  0.05497678],\n",
      "       [ 0.01997549,  0.10038829],\n",
      "       [ 0.03824695,  0.10304374],\n",
      "       [ 0.06208556,  0.0946699 ],\n",
      "       [ 0.12233598, -0.03543108],\n",
      "       [ 0.06428068,  0.07997499],\n",
      "       [-0.00985603, -0.08305734],\n",
      "       [ 0.02025353, -0.09543061],\n",
      "       [-0.09335214, -0.04045701],\n",
      "       [ 0.05653603, -0.03955551],\n",
      "       [-0.0212149 ,  0.05950537],\n",
      "       [ 0.05479232, -0.08865862],\n",
      "       [-0.06271227, -0.00616521],\n",
      "       [-0.08322059, -0.09444351],\n",
      "       [ 0.06355092, -0.115833  ],\n",
      "       [-0.06922431,  0.06530183],\n",
      "       [ 0.05917469,  0.07849282],\n",
      "       [ 0.01401479,  0.01351963],\n",
      "       [ 0.03810755, -0.02678735],\n",
      "       [ 0.04086413, -0.04378407],\n",
      "       [-0.08662243, -0.05540965],\n",
      "       [ 0.06556071, -0.00274828],\n",
      "       [ 0.09472376, -0.03822417],\n",
      "       [-0.03384451,  0.01003726],\n",
      "       [ 0.05466221, -0.06395304],\n",
      "       [-0.04413132, -0.00833847],\n",
      "       [-0.10400607,  0.11038367],\n",
      "       [ 0.05504902,  0.03310199],\n",
      "       [ 0.0670617 , -0.06417592],\n",
      "       [ 0.05163915,  0.07122194],\n",
      "       [ 0.0510257 , -0.05159095],\n",
      "       [-0.07860655, -0.0385831 ],\n",
      "       [-0.0425971 ,  0.08931515],\n",
      "       [ 0.06477237,  0.07649194],\n",
      "       [ 0.0130143 , -0.00678981],\n",
      "       [ 0.09306058, -0.01254047],\n",
      "       [ 0.05911999, -0.02476157],\n",
      "       [-0.05037018,  0.03968317],\n",
      "       [ 0.09468324, -0.09633217],\n",
      "       [ 0.09046121, -0.04414934],\n",
      "       [-0.07896487, -0.01889215],\n",
      "       [-0.05126528,  0.0645128 ],\n",
      "       [-0.02817298, -0.01976723],\n",
      "       [-0.06963074,  0.05060264],\n",
      "       [ 0.04382402,  0.01743763],\n",
      "       [ 0.02111326, -0.056915  ],\n",
      "       [-0.07937434,  0.01911365],\n",
      "       [ 0.0320007 , -0.0966574 ],\n",
      "       [ 0.08609178,  0.08823472],\n",
      "       [-0.05968918,  0.07774922],\n",
      "       [ 0.13485806, -0.06266385],\n",
      "       [ 0.02274571,  0.06322207],\n",
      "       [-0.0048017 ,  0.04118194],\n",
      "       [-0.06662469, -0.07862572],\n",
      "       [-0.0447496 ,  0.06557685],\n",
      "       [ 0.03626973, -0.0363848 ],\n",
      "       [-0.0801399 , -0.04140452],\n",
      "       [-0.0892067 ,  0.04529453],\n",
      "       [-0.04640176,  0.05373997],\n",
      "       [-0.06597373,  0.09341846],\n",
      "       [ 0.03246468, -0.06266692],\n",
      "       [ 0.03325224,  0.04605464],\n",
      "       [-0.088351  ,  0.04260119],\n",
      "       [-0.09440482, -0.05242742],\n",
      "       [ 0.12047318, -0.04484326],\n",
      "       [ 0.0587764 ,  0.05491043],\n",
      "       [ 0.11781491, -0.0247802 ],\n",
      "       [-0.05252584,  0.0948424 ],\n",
      "       [-0.09270789,  0.00290092],\n",
      "       [ 0.02953261,  0.06384847],\n",
      "       [-0.04930701, -0.00180705],\n",
      "       [-0.01726307, -0.00485664],\n",
      "       [-0.00343406,  0.05071843],\n",
      "       [ 0.08734091, -0.04523054],\n",
      "       [-0.00492472, -0.10022837],\n",
      "       [-0.08574443, -0.05268416],\n",
      "       [-0.00830413, -0.11258492],\n",
      "       [ 0.12223374, -0.12711501],\n",
      "       [ 0.05255788, -0.02906406],\n",
      "       [-0.02451387, -0.10607663],\n",
      "       [ 0.05887705,  0.1014686 ],\n",
      "       [ 0.10687933, -0.11090266],\n",
      "       [ 0.01595546, -0.09647645],\n",
      "       [ 0.08522485, -0.09018736],\n",
      "       [ 0.05575566, -0.08034538],\n",
      "       [-0.01402826,  0.09128823],\n",
      "       [-0.01914284,  0.07313996],\n",
      "       [ 0.01201154,  0.0353253 ],\n",
      "       [ 0.02630525, -0.10031029],\n",
      "       [-0.00714251,  0.02835639],\n",
      "       [-0.04130204,  0.04700005],\n",
      "       [ 0.00765558,  0.08880088],\n",
      "       [-0.07652104,  0.10528626],\n",
      "       [-0.03428807, -0.1265489 ],\n",
      "       [-0.08762529,  0.0153307 ],\n",
      "       [ 0.01514399, -0.04275274],\n",
      "       [ 0.08736248,  0.04509316],\n",
      "       [-0.11950286, -0.00858769]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 0.00663124, -0.00663124], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "models = ['d1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7', 'd8', 'd9', 'd10', 'd11', 'd12', 'd13', 'd14', 'd15', 'd16', 'd17', 'd18', 'd19', 'd20']\n",
    "avgWeight = FedAvg(models)\n",
    "print(avgWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNewGlobal(weight):\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "    TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)\n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(TestLabels))\n",
    "    \n",
    "    (x_test) = TestCells\n",
    "    (y_test) = TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_test = x_test.astype('float32')/255\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "#     model.summary()\n",
    "\n",
    "    model.set_weights(weight)\n",
    "\n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./output.h5\")\n",
    "    return scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 13ms/step - loss: 0.3326 - accuracy: 0.8702\n",
      "Loss:  0.3326050043106079\n",
      "Accuracy:  0.8701792359352112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8701792359352112"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testNewGlobal(avgWeight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DataSize</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Weightage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Genesis</th>\n",
       "      <td>1382</td>\n",
       "      <td>0.789969</td>\n",
       "      <td>0.062718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d1</th>\n",
       "      <td>1061</td>\n",
       "      <td>0.874887</td>\n",
       "      <td>0.048151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d2</th>\n",
       "      <td>1050</td>\n",
       "      <td>0.880319</td>\n",
       "      <td>0.047651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d3</th>\n",
       "      <td>1347</td>\n",
       "      <td>0.883940</td>\n",
       "      <td>0.061130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d4</th>\n",
       "      <td>858</td>\n",
       "      <td>0.847909</td>\n",
       "      <td>0.038938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d5</th>\n",
       "      <td>1676</td>\n",
       "      <td>0.901503</td>\n",
       "      <td>0.076061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d6</th>\n",
       "      <td>1166</td>\n",
       "      <td>0.872533</td>\n",
       "      <td>0.052916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d7</th>\n",
       "      <td>813</td>\n",
       "      <td>0.864747</td>\n",
       "      <td>0.036896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d8</th>\n",
       "      <td>1445</td>\n",
       "      <td>0.897157</td>\n",
       "      <td>0.065577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d9</th>\n",
       "      <td>1102</td>\n",
       "      <td>0.891363</td>\n",
       "      <td>0.050011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d10</th>\n",
       "      <td>1396</td>\n",
       "      <td>0.885207</td>\n",
       "      <td>0.063354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d11</th>\n",
       "      <td>1134</td>\n",
       "      <td>0.856962</td>\n",
       "      <td>0.051464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d12</th>\n",
       "      <td>1623</td>\n",
       "      <td>0.899873</td>\n",
       "      <td>0.073656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d13</th>\n",
       "      <td>820</td>\n",
       "      <td>0.833424</td>\n",
       "      <td>0.037214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d14</th>\n",
       "      <td>1041</td>\n",
       "      <td>0.843925</td>\n",
       "      <td>0.047243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d15</th>\n",
       "      <td>565</td>\n",
       "      <td>0.842839</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d16</th>\n",
       "      <td>828</td>\n",
       "      <td>0.868007</td>\n",
       "      <td>0.037577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d17</th>\n",
       "      <td>831</td>\n",
       "      <td>0.876878</td>\n",
       "      <td>0.037713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d18</th>\n",
       "      <td>521</td>\n",
       "      <td>0.832700</td>\n",
       "      <td>0.023644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d19</th>\n",
       "      <td>814</td>\n",
       "      <td>0.877241</td>\n",
       "      <td>0.036941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d20</th>\n",
       "      <td>562</td>\n",
       "      <td>0.816947</td>\n",
       "      <td>0.025505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         DataSize  Accuracy  Weightage\n",
       "Genesis      1382  0.789969   0.062718\n",
       "d1           1061  0.874887   0.048151\n",
       "d2           1050  0.880319   0.047651\n",
       "d3           1347  0.883940   0.061130\n",
       "d4            858  0.847909   0.038938\n",
       "d5           1676  0.901503   0.076061\n",
       "d6           1166  0.872533   0.052916\n",
       "d7            813  0.864747   0.036896\n",
       "d8           1445  0.897157   0.065577\n",
       "d9           1102  0.891363   0.050011\n",
       "d10          1396  0.885207   0.063354\n",
       "d11          1134  0.856962   0.051464\n",
       "d12          1623  0.899873   0.073656\n",
       "d13           820  0.833424   0.037214\n",
       "d14          1041  0.843925   0.047243\n",
       "d15           565  0.842839   0.025641\n",
       "d16           828  0.868007   0.037577\n",
       "d17           831  0.876878   0.037713\n",
       "d18           521  0.832700   0.023644\n",
       "d19           814  0.877241   0.036941\n",
       "d20           562  0.816947   0.025505"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAccuracyDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
