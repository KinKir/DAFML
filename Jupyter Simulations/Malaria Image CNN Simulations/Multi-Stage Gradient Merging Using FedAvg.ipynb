{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\n",
    "import pandas as pd\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(filepath, label):\n",
    "    cells = []\n",
    "    labels = []\n",
    "    file = os.listdir(filepath)\n",
    "    for img in file:\n",
    "        try:\n",
    "            image = cv2.imread(filepath + img)\n",
    "            image_from_array = Image.fromarray(image, 'RGB')\n",
    "            size_image = image_from_array.resize((50, 50))\n",
    "            cells.append(np.array(size_image))\n",
    "            labels.append(label)\n",
    "        except AttributeError as e:\n",
    "            print('Skipping file: ', img, e)\n",
    "    print(len(cells), ' Data Points Read!')\n",
    "    return np.array(cells), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genesis_train(file):\n",
    "    \n",
    "    print('Reading Training Data')\n",
    "    \n",
    "    ParasitizedCells, ParasitizedLabels = readData(file + '/Parasitized/', 1)\n",
    "    UninfectedCells, UninfectedLabels  = readData(file + '/Uninfected/', 0)\n",
    "    Cells = np.concatenate((ParasitizedCells, UninfectedCells))\n",
    "    Labels = np.concatenate((ParasitizedLabels, UninfectedLabels))\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "    TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)\n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    s = np.arange(Cells.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Cells = Cells[s]\n",
    "    Labels = Labels[s]\n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(Labels))\n",
    "    len_data=len(Cells)\n",
    "    print(len_data, ' Data Points')\n",
    "    \n",
    "    (x_train,x_test)=Cells, TestCells\n",
    "    (y_train,y_test)=Labels, TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_train = x_train.astype('float32')/255 \n",
    "    x_test = x_test.astype('float32')/255\n",
    "    train_len=len(x_train)\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "#     model.summary()\n",
    "\n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Fit the model with min batch size as 50[can tune batch size to some factor of 2^power ] \n",
    "    model.fit(x_train, y_train, batch_size=100, epochs=5, verbose=1)\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/global1.h5\")\n",
    "    return len_data, scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_train(file, d, globalId):\n",
    "    \n",
    "    print('Reading Training Data')\n",
    "    \n",
    "    ParasitizedCells, ParasitizedLabels = readData(file + '/Parasitized/', 1)\n",
    "    UninfectedCells, UninfectedLabels  = readData(file + '/Uninfected/', 0)\n",
    "    Cells = np.concatenate((ParasitizedCells, UninfectedCells))\n",
    "    Labels = np.concatenate((ParasitizedLabels, UninfectedLabels))\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "    TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)\n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    s = np.arange(Cells.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    Cells = Cells[s]\n",
    "    Labels = Labels[s]\n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(Labels))\n",
    "    len_data=len(Cells)\n",
    "    print(len_data, ' Data Points')\n",
    "    \n",
    "    (x_train,x_test)=Cells, TestCells\n",
    "    (y_train,y_test)=Labels, TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_train = x_train.astype('float32')/255 \n",
    "    x_test = x_test.astype('float32')/255\n",
    "    train_len=len(x_train)\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_train=keras.utils.to_categorical(y_train,num_classes)\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "    \n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "\n",
    "    model.load_weights(\"./weights/global\"+str(globalId)+\".h5\")\n",
    "    \n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    #Fit the model with min batch size as 50[can tune batch size to some factor of 2^power ] \n",
    "    model.fit(x_train, y_train, batch_size=100, epochs=5, verbose=1)\n",
    "    \n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    model.save(\"./weights/\" + str(d) + \".h5\")\n",
    "    return len_data, scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FedAvg ####\n",
    "\n",
    "def getDataLen(trainingDict):\n",
    "    n = 0\n",
    "    for w in trainingDict:\n",
    "        n += trainingDict[w][0]\n",
    "    print('Total number of data points after this round: ', n)\n",
    "    return n\n",
    "\n",
    "def assignWeights(trainingDf, trainingDict):\n",
    "    n = getDataLen(trainingDict)\n",
    "    trainingDf['Weightage'] = trainingDf['DataSize'].apply(lambda x: x/n)\n",
    "    return trainingDf, n\n",
    "    \n",
    "def scale(weight, scaler):\n",
    "    scaledWeights = []\n",
    "    for i in range(len(weight)):\n",
    "        scaledWeights.append(scaler * weight[i])\n",
    "    return scaledWeights\n",
    "\n",
    "def getScaledWeight(d, scaler):\n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "    \n",
    "    fpath = \"./weights/\"+d+\".h5\"\n",
    "    model.load_weights(fpath)\n",
    "    weight = model.get_weights()\n",
    "    return scale(weight, scaler)\n",
    "\n",
    "def avgWeights(scaledWeights):\n",
    "    avg = list()\n",
    "    for weight_list_tuple in zip(*scaledWeights):\n",
    "        layer_mean = tf.math.reduce_sum(weight_list_tuple, axis=0)\n",
    "        avg.append(layer_mean)\n",
    "    return avg\n",
    "\n",
    "def FedAvg(trainingDict):\n",
    "    trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize', 'Accuracy']) \n",
    "    models = []\n",
    "    for i in trainingDict.keys():\n",
    "#       if 'global' not in i:\n",
    "        models.append(i)\n",
    "    scaledWeights = []\n",
    "    trainingDf, dataLen = assignWeights(trainingDf, trainingDict)\n",
    "    for m in models:\n",
    "        scaledWeights.append(getScaledWeight(m, trainingDf.loc[m]['Weightage']))\n",
    "    fedAvgWeight = avgWeights(scaledWeights)\n",
    "    return fedAvgWeight, dataLen\n",
    "\n",
    "# def FedAvg(trainingDict):\n",
    "#     trainingDf = pd.DataFrame.from_dict(trainingDict, orient='index', columns=['DataSize', 'Accuracy']) \n",
    "#     models = []\n",
    "#     for i in trainingDict.keys():\n",
    "#         if 'global' not in i:\n",
    "#             models.append(i)\n",
    "#     scaledWeights = []\n",
    "#     trainingDf, dataLen = assignWeights(trainingDf, trainingDict)\n",
    "#     for m in models:\n",
    "#         scaledWeights.append(getScaledWeight(m, trainingDf.loc[m]['Weightage']))\n",
    "#     fedAvgWeight = avgWeights(scaledWeights)\n",
    "#     return fedAvgWeight, dataLen\n",
    "\n",
    "def saveModel(weight, n):\n",
    "    \n",
    "    print('Reading Testing Data')\n",
    "    \n",
    "    TestParasitizedCells, TestParasitizedLabels = readData('./input/fed/test/Parasitized/', 1)\n",
    "    TestUninfectedCells, TestUninfectedLabels  = readData('./input/fed/test/Uninfected/', 0)\n",
    "    TestCells = np.concatenate((TestParasitizedCells, TestUninfectedCells))\n",
    "    TestLabels = np.concatenate((TestParasitizedLabels, TestUninfectedLabels))\n",
    "    \n",
    "    \n",
    "    sTest = np.arange(TestCells.shape[0])\n",
    "    np.random.shuffle(sTest)\n",
    "    TestCells = TestCells[sTest]\n",
    "    TestLabels = TestLabels[sTest]\n",
    "    \n",
    "    num_classes=len(np.unique(TestLabels))\n",
    "    \n",
    "    (x_test) = TestCells\n",
    "    (y_test) = TestLabels\n",
    "    \n",
    "    # Since we're working on image data, we normalize data by divinding 255.\n",
    "    x_test = x_test.astype('float32')/255\n",
    "    test_len=len(x_test)\n",
    "    \n",
    "    #Doing One hot encoding as classifier has multiple classes\n",
    "    y_test=keras.utils.to_categorical(y_test,num_classes)\n",
    "\n",
    "    #creating sequential model\n",
    "    model=Sequential()\n",
    "    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500,activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n",
    "    # model.summary()\n",
    "    \n",
    "    model.set_weights(weight)\n",
    "\n",
    "    # compile the model with loss as categorical_crossentropy and using adam optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    scores = model.evaluate(x_test, y_test)\n",
    "    print(\"Loss: \", scores[0])        #Loss\n",
    "    print(\"Accuracy: \", scores[1])    #Accuracy\n",
    "\n",
    "    #Saving Model\n",
    "    fpath = \"./weights/global\"+n+\".h5\"\n",
    "    model.save(fpath)\n",
    "    return scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "686  Data Points Read!\n",
      "696  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1382  Data Points\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 2s 141ms/step - loss: 0.7174 - accuracy: 0.4855\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 2s 140ms/step - loss: 0.6908 - accuracy: 0.5123\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 2s 137ms/step - loss: 0.6853 - accuracy: 0.5984\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 2s 142ms/step - loss: 0.6580 - accuracy: 0.6556\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 0.6340 - accuracy: 0.6505\n",
      "173/173 [==============================] - 3s 16ms/step - loss: 0.6675 - accuracy: 0.5933\n",
      "Loss:  0.6675447225570679\n",
      "Accuracy:  0.5933369398117065\n"
     ]
    }
   ],
   "source": [
    "globalDict = dict()\n",
    "trainingDict = dict()\n",
    "trainingDict['global1'] = genesis_train('./input/fed/genesis')\n",
    "globalDict['global1'] = trainingDict['global1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "528  Data Points Read!\n",
      "533  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1061  Data Points\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 2s 139ms/step - loss: 0.6494 - accuracy: 0.6343\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 2s 148ms/step - loss: 0.6013 - accuracy: 0.6814\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 1s 117ms/step - loss: 0.5792 - accuracy: 0.6993\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 1s 122ms/step - loss: 0.5639 - accuracy: 0.7191\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 2s 143ms/step - loss: 0.5518 - accuracy: 0.7267\n",
      "173/173 [==============================] - 3s 16ms/step - loss: 0.5806 - accuracy: 0.6949\n",
      "Loss:  0.5806125998497009\n",
      "Accuracy:  0.6949121952056885\n",
      "Reading Training Data\n",
      "522  Data Points Read!\n",
      "528  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1050  Data Points\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 3s 230ms/step - loss: 0.6702 - accuracy: 0.5895\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 2s 190ms/step - loss: 0.6449 - accuracy: 0.6248\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 2s 162ms/step - loss: 0.6111 - accuracy: 0.6771\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 0.6042 - accuracy: 0.6752\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 2s 137ms/step - loss: 0.5812 - accuracy: 0.6981\n",
      "173/173 [==============================] - 4s 22ms/step - loss: 0.5845 - accuracy: 0.6891\n",
      "Loss:  0.5845358967781067\n",
      "Accuracy:  0.6891182065010071\n",
      "Reading Training Data\n",
      "692  Data Points Read!\n",
      "655  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1347  Data Points\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 2s 121ms/step - loss: 0.6764 - accuracy: 0.6088\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 2s 121ms/step - loss: 0.6428 - accuracy: 0.6251\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 0.6095 - accuracy: 0.6726\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 2s 123ms/step - loss: 0.5848 - accuracy: 0.6882\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 2s 135ms/step - loss: 0.5716 - accuracy: 0.7023\n",
      "173/173 [==============================] - 3s 17ms/step - loss: 0.5761 - accuracy: 0.7132\n",
      "Loss:  0.5760866403579712\n",
      "Accuracy:  0.7131993770599365\n",
      "Reading Training Data\n",
      "448  Data Points Read!\n",
      "410  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "858  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 0.6283 - accuracy: 0.6387\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 128ms/step - loss: 0.5961 - accuracy: 0.6911\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 123ms/step - loss: 0.5807 - accuracy: 0.6970\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 0.5921 - accuracy: 0.6772\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 125ms/step - loss: 0.5808 - accuracy: 0.7005\n",
      "173/173 [==============================] - 3s 18ms/step - loss: 0.5866 - accuracy: 0.6960\n",
      "Loss:  0.58656907081604\n",
      "Accuracy:  0.6959985494613647\n",
      "Reading Training Data\n",
      "838  Data Points Read!\n",
      "838  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1676  Data Points\n",
      "Epoch 1/5\n",
      "17/17 [==============================] - 2s 135ms/step - loss: 0.6393 - accuracy: 0.6408\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - 2s 127ms/step - loss: 0.6050 - accuracy: 0.6832\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - 2s 143ms/step - loss: 0.6042 - accuracy: 0.6665\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - 3s 148ms/step - loss: 0.5719 - accuracy: 0.7076\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - 2s 139ms/step - loss: 0.5382 - accuracy: 0.7399\n",
      "173/173 [==============================] - 3s 15ms/step - loss: 0.5422 - accuracy: 0.7308\n",
      "Loss:  0.5422363877296448\n",
      "Accuracy:  0.730762243270874\n"
     ]
    }
   ],
   "source": [
    "trainingDict['d1'] = local_train('./input/fed/d1', 'd1', 1)\n",
    "trainingDict['d2'] = local_train('./input/fed/d2', 'd2', 1)\n",
    "trainingDict['d3'] = local_train('./input/fed/d3', 'd3', 1)\n",
    "trainingDict['d4'] = local_train('./input/fed/d4', 'd4', 1)\n",
    "trainingDict['d5'] = local_train('./input/fed/d5', 'd5', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global1': (1382, 0.5933369398117065),\n",
       " 'd1': (1061, 0.6949121952056885),\n",
       " 'd2': (1050, 0.6891182065010071),\n",
       " 'd3': (1347, 0.7131993770599365),\n",
       " 'd4': (858, 0.6959985494613647),\n",
       " 'd5': (1676, 0.730762243270874)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points after this round:  7374\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 14ms/step - loss: 0.5866 - accuracy: 0.6860\n",
      "Loss:  0.5865991711616516\n",
      "Accuracy:  0.6860402226448059\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = FedAvg(trainingDict)\n",
    "trainingDict = {}\n",
    "trainingDict['global2'] = (dataLen, saveModel(NewGlobal, '2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "599  Data Points Read!\n",
      "567  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1166  Data Points\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 0.6476 - accuracy: 0.6432\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 2s 135ms/step - loss: 0.5989 - accuracy: 0.6913\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 2s 167ms/step - loss: 0.5780 - accuracy: 0.7058\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 0.5517 - accuracy: 0.7161\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 2s 153ms/step - loss: 0.5234 - accuracy: 0.7307\n",
      "173/173 [==============================] - 3s 16ms/step - loss: 0.5193 - accuracy: 0.7512\n",
      "Loss:  0.5192921161651611\n",
      "Accuracy:  0.7512221336364746\n",
      "Reading Training Data\n",
      "418  Data Points Read!\n",
      "395  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "813  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.6534 - accuracy: 0.6236\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.6199 - accuracy: 0.6494\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.5980 - accuracy: 0.6777\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 78ms/step - loss: 0.5616 - accuracy: 0.7134\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 75ms/step - loss: 0.5440 - accuracy: 0.7355\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5597 - accuracy: 0.7165\n",
      "Loss:  0.5596891045570374\n",
      "Accuracy:  0.7164584398269653\n",
      "Reading Training Data\n",
      "716  Data Points Read!\n",
      "729  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1445  Data Points\n",
      "Epoch 1/5\n",
      "15/15 [==============================] - 1s 100ms/step - loss: 0.6032 - accuracy: 0.6796\n",
      "Epoch 2/5\n",
      "15/15 [==============================] - 1s 99ms/step - loss: 0.5659 - accuracy: 0.7121\n",
      "Epoch 3/5\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.5280 - accuracy: 0.7481\n",
      "Epoch 4/5\n",
      "15/15 [==============================] - 3s 168ms/step - loss: 0.5019 - accuracy: 0.7536\n",
      "Epoch 5/5\n",
      "15/15 [==============================] - 2s 148ms/step - loss: 0.4818 - accuracy: 0.7737\n",
      "173/173 [==============================] - 2s 12ms/step - loss: 0.5174 - accuracy: 0.7326\n",
      "Loss:  0.517432689666748\n",
      "Accuracy:  0.7325728535652161\n",
      "Reading Training Data\n",
      "530  Data Points Read!\n",
      "572  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1102  Data Points\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 0.6523 - accuracy: 0.6334\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 1s 94ms/step - loss: 0.6058 - accuracy: 0.6697\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 1s 117ms/step - loss: 0.5890 - accuracy: 0.6987\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 1s 89ms/step - loss: 0.5702 - accuracy: 0.7260\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 1s 99ms/step - loss: 0.5891 - accuracy: 0.6824\n",
      "173/173 [==============================] - 2s 11ms/step - loss: 0.5705 - accuracy: 0.7212\n",
      "Loss:  0.5705460906028748\n",
      "Accuracy:  0.7211660146713257\n",
      "Reading Training Data\n",
      "695  Data Points Read!\n",
      "701  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1396  Data Points\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 1s 100ms/step - loss: 0.6085 - accuracy: 0.6734\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 1s 90ms/step - loss: 0.5615 - accuracy: 0.7242\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 1s 88ms/step - loss: 0.5286 - accuracy: 0.7450\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 0.5006 - accuracy: 0.7715\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 2s 128ms/step - loss: 0.4748 - accuracy: 0.7772\n",
      "173/173 [==============================] - 4s 25ms/step - loss: 0.5264 - accuracy: 0.7344\n",
      "Loss:  0.5264465808868408\n",
      "Accuracy:  0.7343834638595581\n"
     ]
    }
   ],
   "source": [
    "globalDict['global2'] = trainingDict['global2']\n",
    "trainingDict['d6'] = local_train('./input/fed/d6', 'd6', 2)\n",
    "trainingDict['d7'] = local_train('./input/fed/d7', 'd7', 2)\n",
    "trainingDict['d8'] = local_train('./input/fed/d8', 'd8', 2)\n",
    "trainingDict['d9'] = local_train('./input/fed/d9', 'd9', 2)\n",
    "trainingDict['d10'] = local_train('./input/fed/d10', 'd10', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points after this round:  13296\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5690 - accuracy: 0.6913\n",
      "Loss:  0.5690057873725891\n",
      "Accuracy:  0.6912909746170044\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = FedAvg(trainingDict)\n",
    "trainingDict = {}\n",
    "trainingDict['global3'] = (dataLen, saveModel(NewGlobal, '3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "557  Data Points Read!\n",
      "577  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1134  Data Points\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 1s 91ms/step - loss: 0.6326 - accuracy: 0.6517\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 1s 98ms/step - loss: 0.5715 - accuracy: 0.7152\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 1s 80ms/step - loss: 0.5329 - accuracy: 0.7416\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 1s 83ms/step - loss: 0.5021 - accuracy: 0.7637\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 1s 82ms/step - loss: 0.4683 - accuracy: 0.7778\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 0.5010 - accuracy: 0.7637\n",
      "Loss:  0.5010061860084534\n",
      "Accuracy:  0.763715386390686\n",
      "Reading Training Data\n",
      "827  Data Points Read!\n",
      "796  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1623  Data Points\n",
      "Epoch 1/5\n",
      "17/17 [==============================] - 2s 92ms/step - loss: 0.5740 - accuracy: 0.7055\n",
      "Epoch 2/5\n",
      "17/17 [==============================] - 2s 92ms/step - loss: 0.5176 - accuracy: 0.7486\n",
      "Epoch 3/5\n",
      "17/17 [==============================] - 2s 94ms/step - loss: 0.4866 - accuracy: 0.7628\n",
      "Epoch 4/5\n",
      "17/17 [==============================] - 2s 97ms/step - loss: 0.4350 - accuracy: 0.7930\n",
      "Epoch 5/5\n",
      "17/17 [==============================] - 2s 110ms/step - loss: 0.3969 - accuracy: 0.8275\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.4238 - accuracy: 0.7880\n",
      "Loss:  0.4237591326236725\n",
      "Accuracy:  0.7879775762557983\n",
      "Reading Training Data\n",
      "395  Data Points Read!\n",
      "425  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "820  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.5545 - accuracy: 0.7317\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.5277 - accuracy: 0.7390\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.5369 - accuracy: 0.7390\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 89ms/step - loss: 0.4812 - accuracy: 0.7927\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.4624 - accuracy: 0.7915\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.5613 - accuracy: 0.7023\n",
      "Loss:  0.5612853765487671\n",
      "Accuracy:  0.7023357152938843\n",
      "Reading Training Data\n",
      "513  Data Points Read!\n",
      "528  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "1041  Data Points\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 1s 88ms/step - loss: 0.6091 - accuracy: 0.6619\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 1s 85ms/step - loss: 0.5690 - accuracy: 0.7061\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 1s 79ms/step - loss: 0.5236 - accuracy: 0.7454\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 1s 80ms/step - loss: 0.5097 - accuracy: 0.7608\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 1s 78ms/step - loss: 0.4849 - accuracy: 0.7723\n",
      "173/173 [==============================] - 1s 8ms/step - loss: 0.4974 - accuracy: 0.7659\n",
      "Loss:  0.497401624917984\n",
      "Accuracy:  0.7658880949020386\n",
      "Reading Training Data\n",
      "284  Data Points Read!\n",
      "281  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "565  Data Points\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 1s 85ms/step - loss: 0.6122 - accuracy: 0.6637\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 83ms/step - loss: 0.5662 - accuracy: 0.6956\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 1s 86ms/step - loss: 0.5498 - accuracy: 0.7257\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 1s 113ms/step - loss: 0.5314 - accuracy: 0.7310\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 81ms/step - loss: 0.5088 - accuracy: 0.7451\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.5299 - accuracy: 0.7367\n",
      "Loss:  0.5298902988433838\n",
      "Accuracy:  0.7367372512817383\n"
     ]
    }
   ],
   "source": [
    "globalDict['global3'] = trainingDict['global3']\n",
    "trainingDict['d11'] = local_train('./input/fed/d11', 'd11', 3)\n",
    "trainingDict['d12'] = local_train('./input/fed/d12', 'd12', 3)\n",
    "trainingDict['d13'] = local_train('./input/fed/d13', 'd13', 3)\n",
    "trainingDict['d14'] = local_train('./input/fed/d14', 'd14', 3)\n",
    "trainingDict['d15'] = local_train('./input/fed/d15', 'd15', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points after this round:  18479\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5484 - accuracy: 0.7090\n",
      "Loss:  0.548358142375946\n",
      "Accuracy:  0.7090349197387695\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = FedAvg(trainingDict)\n",
    "trainingDict = {}\n",
    "trainingDict['global4'] = (dataLen, saveModel(NewGlobal, '4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data\n",
      "412  Data Points Read!\n",
      "416  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "828  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 0.5760 - accuracy: 0.7065\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 86ms/step - loss: 0.5416 - accuracy: 0.7415\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 83ms/step - loss: 0.4957 - accuracy: 0.7669\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.4890 - accuracy: 0.7524\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 81ms/step - loss: 0.4670 - accuracy: 0.7790\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5197 - accuracy: 0.7284\n",
      "Loss:  0.5196879506111145\n",
      "Accuracy:  0.7284084558486938\n",
      "Reading Training Data\n",
      "417  Data Points Read!\n",
      "414  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "831  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 101ms/step - loss: 0.5654 - accuracy: 0.7184\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 87ms/step - loss: 0.5414 - accuracy: 0.7389\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 111ms/step - loss: 0.4889 - accuracy: 0.7702\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 0.4611 - accuracy: 0.7894\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 0.4296 - accuracy: 0.7906\n",
      "173/173 [==============================] - 1s 9ms/step - loss: 0.4628 - accuracy: 0.7869\n",
      "Loss:  0.46278688311576843\n",
      "Accuracy:  0.7868911623954773\n",
      "Reading Training Data\n",
      "269  Data Points Read!\n",
      "252  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "521  Data Points\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 0s 70ms/step - loss: 0.5372 - accuracy: 0.7447\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 72ms/step - loss: 0.5519 - accuracy: 0.7274\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.5251 - accuracy: 0.7428\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 68ms/step - loss: 0.4982 - accuracy: 0.7658\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 67ms/step - loss: 0.4752 - accuracy: 0.7869\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.5007 - accuracy: 0.7594\n",
      "Loss:  0.5006759166717529\n",
      "Accuracy:  0.7593699097633362\n",
      "Reading Training Data\n",
      "407  Data Points Read!\n",
      "407  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "814  Data Points\n",
      "Epoch 1/5\n",
      "9/9 [==============================] - 1s 80ms/step - loss: 0.5805 - accuracy: 0.6880\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.5972 - accuracy: 0.6855\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.5430 - accuracy: 0.7371\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.5110 - accuracy: 0.7568\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.4786 - accuracy: 0.7826\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.4966 - accuracy: 0.7601\n",
      "Loss:  0.496591180562973\n",
      "Accuracy:  0.760094165802002\n",
      "Reading Training Data\n",
      "286  Data Points Read!\n",
      "276  Data Points Read!\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "562  Data Points\n",
      "Epoch 1/5\n",
      "6/6 [==============================] - 0s 76ms/step - loss: 0.5974 - accuracy: 0.6726\n",
      "Epoch 2/5\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.5649 - accuracy: 0.7206\n",
      "Epoch 3/5\n",
      "6/6 [==============================] - 0s 77ms/step - loss: 0.5149 - accuracy: 0.7758\n",
      "Epoch 4/5\n",
      "6/6 [==============================] - 0s 73ms/step - loss: 0.5007 - accuracy: 0.7509\n",
      "Epoch 5/5\n",
      "6/6 [==============================] - 0s 71ms/step - loss: 0.4991 - accuracy: 0.7562\n",
      "173/173 [==============================] - 2s 9ms/step - loss: 0.4967 - accuracy: 0.7626\n",
      "Loss:  0.49665457010269165\n",
      "Accuracy:  0.7626290321350098\n"
     ]
    }
   ],
   "source": [
    "globalDict['global4'] = trainingDict['global4']\n",
    "trainingDict['d16'] = local_train('./input/fed/d16', 'd16', 4)\n",
    "trainingDict['d17'] = local_train('./input/fed/d17', 'd17', 4)\n",
    "trainingDict['d18'] = local_train('./input/fed/d18', 'd18', 4)\n",
    "trainingDict['d19'] = local_train('./input/fed/d19', 'd19', 4)\n",
    "trainingDict['d20'] = local_train('./input/fed/d20', 'd20', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data points after this round:  22035\n",
      "Reading Testing Data\n",
      "2740  Data Points Read!\n",
      "2783  Data Points Read!\n",
      "173/173 [==============================] - 2s 10ms/step - loss: 0.4009 - accuracy: 0.8142\n",
      "Loss:  0.4008854031562805\n",
      "Accuracy:  0.8142313957214355\n"
     ]
    }
   ],
   "source": [
    "NewGlobal, dataLen = FedAvg(trainingDict)\n",
    "trainingDict = {}\n",
    "trainingDict['global5'] = (dataLen, saveModel(NewGlobal, '5'))\n",
    "globalDict['global5'] = trainingDict['global5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global1': (1382, 0.5933369398117065),\n",
       " 'global2': (7374, 0.6860402226448059),\n",
       " 'global3': (13296, 0.6912909746170044),\n",
       " 'global4': (18479, 0.7090349197387695),\n",
       " 'global5': (22035, 0.8142313957214355)}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
